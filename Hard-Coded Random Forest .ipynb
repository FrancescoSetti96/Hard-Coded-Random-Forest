{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c37756",
   "metadata": {},
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ca18e",
   "metadata": {},
   "source": [
    "The goal of this project is building a random forest classifier from scratch, using only numpy and pandas. We will start by building a decision tree classifier, and we will later use this to build a random forest classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b04ab8",
   "metadata": {},
   "source": [
    "# Import Toy Datasets and Test Scikit-Learn's Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81ac00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c8f6d",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97833e2e",
   "metadata": {},
   "source": [
    "Let's import the iris dataset as a toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcab395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee2795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.333333\n",
       "1    0.333333\n",
       "2    0.333333\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0feaf477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50050cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a621173",
   "metadata": {},
   "source": [
    "As we can see, the advantage of using the iris dataset is that the size of the dataset is relatively small, and so the training time of our algorithm shouldn't be too large. Also, this dataset has 3 (balanced) target classes, and so we check whether our model can handle a non-binary target. However, the dataset only contains continuous variables. Since we want to make sure that out algorithm can handle discrete variables, let's make one continuous variable a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d81806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      target  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2689756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sepal_length_cm', 'sepal_width_cm', 'petal_length_cm',\n",
      "       'petal_width_cm', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "new_column_names = {'sepal length (cm)': 'sepal_length_cm',\n",
    "                    'sepal width (cm)': 'sepal_width_cm',\n",
    "                    'petal length (cm)': 'petal_length_cm',\n",
    "                    'petal width (cm)': 'petal_width_cm'}\n",
    "\n",
    "\n",
    "iris_df = iris_df.rename(columns=new_column_names)\n",
    "\n",
    "print(iris_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb53f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length_cm</th>\n",
       "      <th>sepal_width_cm</th>\n",
       "      <th>petal_length_cm</th>\n",
       "      <th>petal_width_cm</th>\n",
       "      <th>target</th>\n",
       "      <th>wide_sepal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length_cm  sepal_width_cm  petal_length_cm  petal_width_cm  target  \\\n",
       "0              5.1             3.5              1.4             0.2       0   \n",
       "1              4.9             3.0              1.4             0.2       0   \n",
       "2              4.7             3.2              1.3             0.2       0   \n",
       "3              4.6             3.1              1.5             0.2       0   \n",
       "4              5.0             3.6              1.4             0.2       0   \n",
       "\n",
       "   wide_sepal  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['wide_sepal']=1\n",
    "iris_df['sepal_width_cm']<iris_df['sepal_width_cm'].mean()\n",
    "iris_df.loc[iris_df['sepal_width_cm']<iris_df['sepal_width_cm'].mean(),'wide_sepal']=0\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c533ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris_df.drop('sepal_width_cm', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a88ffa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.553333\n",
       "1    0.446667\n",
       "Name: wide_sepal, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wide_sepal'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ee1d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sepal_length_cm  150 non-null    float64\n",
      " 1   petal_length_cm  150 non-null    float64\n",
      " 2   petal_width_cm   150 non-null    float64\n",
      " 3   target           150 non-null    int32  \n",
      " 4   wide_sepal       150 non-null    int64  \n",
      "dtypes: float64(3), int32(1), int64(1)\n",
      "memory usage: 5.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e7c3c",
   "metadata": {},
   "source": [
    "Thus, we used the sepal_width_cm column to create a new binary column named wide_sepal. Specifically, this variable is 0 if the width of a flower's sepal is smaller than the average of the dataset, and it is 1 otherwise. \n",
    "\n",
    "We can now proceed to separate the dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ee22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df.drop('target', axis=1), df.target\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7add51",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c2375",
   "metadata": {},
   "source": [
    "The iris dataset is a very useful basic toy dataset. However, we would also like a toy dataset with a larger number of rows and columns so that it that potential performance issues in our model are easier to identify. To do this, we will use the breast cancel dataset as our second toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ef331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df_cancer = pd.DataFrame(X, columns=cancer.feature_names)\n",
    "df_cancer['target'] = y\n",
    "\n",
    "# Rename the columns to replace spaces with underscores\n",
    "df_cancer.columns = df_cancer.columns.str.replace(' ', '_')\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(df_cancer.drop('target', axis=1), df_cancer['target'], test_size=0.2, random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a9f47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453163b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b572e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean_radius              569 non-null    float64\n",
      " 1   mean_texture             569 non-null    float64\n",
      " 2   mean_perimeter           569 non-null    float64\n",
      " 3   mean_area                569 non-null    float64\n",
      " 4   mean_smoothness          569 non-null    float64\n",
      " 5   mean_compactness         569 non-null    float64\n",
      " 6   mean_concavity           569 non-null    float64\n",
      " 7   mean_concave_points      569 non-null    float64\n",
      " 8   mean_symmetry            569 non-null    float64\n",
      " 9   mean_fractal_dimension   569 non-null    float64\n",
      " 10  radius_error             569 non-null    float64\n",
      " 11  texture_error            569 non-null    float64\n",
      " 12  perimeter_error          569 non-null    float64\n",
      " 13  area_error               569 non-null    float64\n",
      " 14  smoothness_error         569 non-null    float64\n",
      " 15  compactness_error        569 non-null    float64\n",
      " 16  concavity_error          569 non-null    float64\n",
      " 17  concave_points_error     569 non-null    float64\n",
      " 18  symmetry_error           569 non-null    float64\n",
      " 19  fractal_dimension_error  569 non-null    float64\n",
      " 20  worst_radius             569 non-null    float64\n",
      " 21  worst_texture            569 non-null    float64\n",
      " 22  worst_perimeter          569 non-null    float64\n",
      " 23  worst_area               569 non-null    float64\n",
      " 24  worst_smoothness         569 non-null    float64\n",
      " 25  worst_compactness        569 non-null    float64\n",
      " 26  worst_concavity          569 non-null    float64\n",
      " 27  worst_concave_points     569 non-null    float64\n",
      " 28  worst_symmetry           569 non-null    float64\n",
      " 29  worst_fractal_dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int32  \n",
      "dtypes: float64(30), int32(1)\n",
      "memory usage: 135.7 KB\n"
     ]
    }
   ],
   "source": [
    "df_cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35faec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.627417\n",
       "0    0.372583\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6084f5",
   "metadata": {},
   "source": [
    "As we can see, this dataset has more rows and way more columns than the iris dataset, and the target has fairly balanced classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb699e",
   "metadata": {},
   "source": [
    "### Scikit-Learn's Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a5672",
   "metadata": {},
   "source": [
    "We would now like to briefly review scikit-learn's decision tree classifier and assess its training time on our toy datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "77ded14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 0.0045146942138671875 seconds\n",
      "Iris prediction time: 0.007011890411376953 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_iris = DecisionTreeClassifier()\n",
    "\n",
    "t_1 = time.time()\n",
    "clf_iris.fit(X_train_iris, y_train_iris)\n",
    "t_2 = time.time()\n",
    "y_pred_iris = clf_iris.predict(X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_tree_training_time = t_2 - t_1\n",
    "iris_tree_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_tree_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_tree_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "291491ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 0.013775825500488281 seconds\n",
      "Cancer prediction time: 0.004000186920166016 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_cancer = DecisionTreeClassifier()\n",
    "\n",
    "t_1 = time.time()\n",
    "clf_cancer.fit(X_train_cancer, y_train_cancer)\n",
    "t_2 = time.time()\n",
    "y_pred_cancer = clf_cancer.predict(X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_tree_training_time = t_2 - t_1\n",
    "cancer_tree_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_tree_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_tree_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5326e",
   "metadata": {},
   "source": [
    "Thus, both modes are trained in much less than one second and the training time for the iris dataset is one order of magnitude smaller than the one for the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbe90b",
   "metadata": {},
   "source": [
    "The default hyperparameters for scikit-learn's decision tree classifier are \n",
    "> **criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0** \n",
    "\n",
    "Of these, we are only interested in \n",
    "> **criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, max_leaf_nodes=None**\n",
    "\n",
    "Specifically:\n",
    "- **criterion='gini'** indicates what criterion to use to evaluate the quality of a split \n",
    "- **splitter='best'** indicates that the best split is performed\n",
    "- **max_depth=None** indicates that there is no limit to the depth of the tree; if another value is selected, all nodes that reach that depth will become leaves\n",
    "- **min_samples_split=2** sets the minimum number of data points that an internal node must have in order to be split; in other words, if a node contains a number of data points smaller than **min_samples_split**, then the node is a leaf and won't be split further\n",
    "- **min_samples_leaf=1** sets the minimum unmber of data points that a leaf must contain; in this case, **min_samples_leaf=1** and so any leaf that is not empty is allowed\n",
    "- **max_features=None** indicates the number of features to be used for a split; in this case, since **max_features=None**, all features will be used\n",
    "- **max_leaf_nodes=None** sets the maximum number of leaves that the tree can have; in this case there is no limit, and the same will be true for our tree\n",
    "\n",
    "There are other hyperparameters that can be useful but we will not need them. For example, **ccp_alpha** is a hyperparameter used for pruning the tree. However, we will not need to prune our decision trees since we will use them to build a random forest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c818199",
   "metadata": {},
   "source": [
    "# Building the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1def1",
   "metadata": {},
   "source": [
    "As we saw in the previous section, scikit-learn's decision tree classifier has several hyperparameters, and needs training data (not containing the target) and the corresponding target in order to be fitted. Once the model has been fitted, it needs new data to make predictions. Our goal is to write a function whose variables are the training data, the training target, and the hyperparameters needed to set stopping conditions and to put some constraints on the tree. The output of this function will be a set of rules that describe how the data was split into leaves. Then, we will write a function that uses the leaves produced by the fitted model and a new dataset in order to make predictions. The predictions produced by this function will be organized in a NumPy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8073f88",
   "metadata": {},
   "source": [
    "In order to fit the tree, we will need a few functions:\n",
    "- A function to calcualte the Gini index\n",
    "- A function to find the best split value in a given column\n",
    "- A function to determine the best split in the data contained in an internal node "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8b58b",
   "metadata": {},
   "source": [
    "Let's start by coding a function that calculates the Gini index for a potential node. Given a subset of the dataset, the Gini index $G$ is $$G = 1 - \\sum_{k=1}^K p_k^2$$ where $K$ is the number of classes in the target variable and $p_k$ is the proportion of data points that belong to class $k$. If a node is pure, meaning that all data points belong to the same class, the Gini index is zero. On the contrary, it gets close to $1$ if the node is impure, so that $p_k$ is small for all $k$'s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ad9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGiniIndex(y):\n",
    "    #calculate gini index for values in a node\n",
    "    probSquared = np.array([])\n",
    "    y.shape[0]\n",
    "    for i in np.unique(y):\n",
    "        prob = np.divide(np.count_nonzero(y == i),y.shape[0])\n",
    "        probSquared=np.append(probSquared,np.square(prob))\n",
    "    return 1-np.sum(probSquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c3c8e",
   "metadata": {},
   "source": [
    "Next, we need a function that uses the Gini index to perform the best split in a given column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0347dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestColumnSplit(column,y):\n",
    "    # given a column, it returns the value of the best split \n",
    "    # and its weighted Gini index\n",
    "    best_cut_value = 0\n",
    "    weigted_Gini_of_best_cut = 2\n",
    "    for i in np.unique(column):\n",
    "        y_leaf_1 = y[column<i]\n",
    "        y_leaf_2 = y[column>=i]\n",
    "        gini_1 = getGiniIndex(y_leaf_1)\n",
    "        gini_2 = getGiniIndex(y_leaf_2)\n",
    "        weighted_Gini = gini_1*(np.divide(y_leaf_1.shape[0],y.shape[0]))+gini_2*(np.divide(y_leaf_2.shape[0],y.shape[0]))\n",
    "        if weighted_Gini < weigted_Gini_of_best_cut:\n",
    "            weigted_Gini_of_best_cut = weighted_Gini\n",
    "            best_cut_value = i\n",
    "    return (best_cut_value,weigted_Gini_of_best_cut,column.name)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd31e6",
   "metadata": {},
   "source": [
    "This function starts with a weighted Gini index value of 2 (which is higher than the maximum possible value of 1) and a best cut value of 0. Then it iterates through all values in the column and it calculates the weighted Gini index for the two potential nodes the split would give. If it finds a weighted Gini index lower of the current lowest weighted Gini, it updates this value and stores the split value that produced the new best weighted Gini index. The function returns the value at which the data should be split, the weighted Gini index of this split, and the column name. \n",
    "\n",
    "One potential issue of this function is that it needs to iterate through all values contained in the column. If the column is a binary variable with only two values, this function executes really fast. However, if the column is a continuous variable, the function might be very slow because all entries might be unique values, and so the function might have to check hundreds or thousands of potential splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cc59d",
   "metadata": {},
   "source": [
    "Next, we use the **bestColumnSplit** function to obtain the best split in a subset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "309b639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSubsetSplit(x,y):\n",
    "    results = np.array((np.nan,2,np.nan))\n",
    "    for column in x.columns:\n",
    "        a=np.array(bestColumnSplit(x[str(column)],y))\n",
    "        results = np.vstack((results,a))\n",
    "    row_best_cut = results[:,1].argmin()\n",
    "    # return best cut value and column name\n",
    "    return (float(results[row_best_cut,0]),results[row_best_cut,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e760a",
   "metadata": {},
   "source": [
    "As we can see in the above cell, this function uses the **bestColumnSplit** on each column and it stores its outputs in a NumPy array with 3 columns. At the end, it returns the row corresponding to the cut that produced the purest nodes (i.e. the ones that have the lowest weighted Gini index). Like **bestColumnSplit**, **bestSubsetSplit** returns the best cut value, the weighted Gini index that the split produces, and the name of the variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1b6bd",
   "metadata": {},
   "source": [
    "Now we can write the function to train the decision tree. However, it is convenient to first define a few more functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bb0962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_not_in_leaves(subset, leaves):\n",
    "    for leaf in leaves:\n",
    "        if subset[0].equals(leaf[0]) and subset[1]==leaf[1] and subset[2]==leaf[2]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c368e",
   "metadata": {},
   "source": [
    "**subset_not_in_leaves** checks whether a subset of the data is contained in the list of leaves. It is convenient to define a function to do this because we need to check a few conditions. The function returns `True` if the subset is *not* in the list of leaves and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3fb1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subset_from_nodes(subset,nodes):\n",
    "    # This only removes the subset once. Ideally, nodes should not contain the same subset more than once.\n",
    "    for i, node in enumerate(nodes):\n",
    "        if subset[0].equals(node[0]) and subset[1]==node[1] and subset[2]==node[2]:\n",
    "            nodes.pop(i)\n",
    "    return nodes\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e423a",
   "metadata": {},
   "source": [
    "Similarly to **subset_not_in_leaves**, **remove_subset_from_nodes** checks whether a subset of the data is in the list of internal nodes. If so, the node is removed from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "278ba867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_leaves(leaves):\n",
    "    new_leaves = []\n",
    "    for leaf in leaves: \n",
    "        new_leaf = (leaf[3], leaf[1], leaf[2])\n",
    "        new_leaves.append(new_leaf)\n",
    "    return new_leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041470e8",
   "metadata": {},
   "source": [
    "**restructure_leaves** changes the order in which the information regarding a leaf is stored in a tuple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbde01a",
   "metadata": {},
   "source": [
    "Finally, we can define the function we will need to fit a classification decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "167be2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(x,y,max_depth,min_samples_split,min_samples_leaf):\n",
    "    df = x.copy()\n",
    "    # subset = (mask, prediction, depth, query)\n",
    "    potential_nodes = [(pd.Series([np.nan,np.nan]),y.value_counts().idxmax(), 0, 'True')]\n",
    "    leaves = []\n",
    "    while len(potential_nodes) != 0:\n",
    "        for subset in potential_nodes:\n",
    "            if subset[2]==0:\n",
    "                dropped_columns=[]\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit(df.drop(dropped_columns,axis=1),y)\n",
    "                    mask_1 = df[cut_variable] < cut_value\n",
    "                    mask_2 = df[cut_variable] >= cut_value\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns.append(cut_variable)\n",
    "                        if dropped_columns == df.columns.tolist():\n",
    "                                return \"No branches found. Please, try to increase the value of min_samples_leaf.\"\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = 1\n",
    "                query_1 = '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                query_2 = '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            else:\n",
    "                dropped_columns=[]\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit(df[subset[0]].drop(dropped_columns,axis=1),y[subset[0]]) \n",
    "                    mask_1 = (subset[0]) & (df[cut_variable] < cut_value)\n",
    "                    mask_2 = (subset[0]) & (df[cut_variable] >= cut_value)\n",
    "                    query_1 = subset[3] + '&' + '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                    query_2 = subset[3] + '&' + '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns.append(cut_variable)\n",
    "                        if dropped_columns == df.columns.tolist():\n",
    "                            leaves.append(subset)\n",
    "                            #potential_nodes.remove(subset)\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = subset[2]+1\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            # check if the new nodes are leaves \n",
    "            if subset_not_in_leaves(subset,leaves):\n",
    "                if node_1[2] == max_depth or df[mask_1].shape[0] <= min_samples_split or y[mask_1].unique().shape[0]==1:\n",
    "                    leaves.append(node_1)\n",
    "                else:\n",
    "                    potential_nodes.append(node_1)\n",
    "                if node_2[2] == max_depth or df[mask_2].shape[0] <= min_samples_split or y[mask_2].unique().shape[0]==1:\n",
    "                    leaves.append(node_2)\n",
    "                else:\n",
    "                    potential_nodes.append(node_2)\n",
    "            potential_nodes = remove_subset_from_nodes(subset,potential_nodes)\n",
    "    return restructure_leaves(leaves) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f46b0f",
   "metadata": {},
   "source": [
    "The variables of this function are:\n",
    "- **x**: the training data, organized in a pandas dataframe \n",
    "- **y**: the training target stored in a pandas series \n",
    "- **max_depth**: hyperparameter constraining the maximum depth of the decision tree\n",
    "- **min_samples_split**: hyperparameter setting a limit to the minimum number of data points that can be contained in an internal node\n",
    "- **min_samples_leaf**: hyperparameter indicating the minimum number of data points that can be contained in a leaf\n",
    "It is worth pointing out that our function cannot handle nominal categorical variables. Such variables should be hot-encoded into boolean variables. \n",
    "\n",
    "By the way the function is built, we have:\n",
    "- **criterion='gini'**\n",
    "- **splitter='best'**\n",
    "- **max_features=None**\n",
    "- **max_leaf_nodes=None**\n",
    "\n",
    "The function splits the data into subsets, which have the following basic structure: $$ \\text{subset} = (\\text{mask}, \\text{prediction}, \\text{depth}, \\text{query}) $$\n",
    "where\n",
    "- **mask** is a condition such as `df['column_1'] < 5`. This gives a NumPy array containing `True` and `False`. Although masks are convenient to split the data, they only work on datasets that have the same number of rows. Thus, we cannot use masks obtained from the training data to split the test data. This is why we also need **query**.\n",
    "- **prediction** is the category that has the highest count in the subset of the data. If the subset is a leaf, this is the prediction that the model would make for data that belong to the leaf.\n",
    "- **depth** is the depth of the subset. We need to store this value in case we want to contstrain the maximum depth of the decision tree.\n",
    "- **query** is similar to **mask** but it is a string rather than a Numpy array. For example, `df['column_1'] < 5` is a **mask** while `\"df['column_1'] < 5\"` is a **query**. The advantage of storing masks as queries is that thanks to queries it is easy to see how the data has been splitted. In additions, queries can be used to make predictions on any dataset that has the same columns as the training dataset. \n",
    "\n",
    "The basic way this function works is by dividing the data into subsets, which are part of two possible lists: `potential_nodes` or `leaves`. At the beginning, `potential_nodes` contains the whole training data and `leaves` is empty. At each iteration, the function splits the data using the Gini index as a criterion to make the best *allowed* split among all internal nodes contained in `potential_nodes`. Notice that the best split might not be allowed depending on the value of **min_samples_leaf**. Once a subset is split into two new nodes, the function checks whether either node is actually a leaf. Specifically, a node is a leaf if at least one of these conditions is true:\n",
    "- the node's depth is equl to the value of **max_depth**\n",
    "- the number of data samples in the node is smaller than or equal to **min_samples_split**, meaning that the node cannot be further splitted \n",
    "- the node is pure, and so we don't need to split it again since the predictions would be the same\n",
    "If a node is a leaf, then it is added to `leaves`, otherwise it is added to `potential_nodes`. The function stops when there are no more elements in `potential_nodes`, meaning that all the data has been split into subsets that are in `leaves`. The function returnes the subsets in `leaves` structured in the following way: $$ \\text{leaf} = (\\text{query}, \\text{prediction}, \\text{depth})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d4d45",
   "metadata": {},
   "source": [
    "Now that we have a function to fit the decision tree, we can write a function to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85685342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(leaves,x):\n",
    "    df=x.copy()\n",
    "    df['y']=np.nan\n",
    "    for leaf in leaves:\n",
    "        mask = df.eval(leaf[0])\n",
    "        df.loc[mask, 'y'] = leaf[1]\n",
    "    return df['y'].values   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0591f",
   "metadata": {},
   "source": [
    "**model_predict** has two variables:\n",
    "- **leaves**: this is the output of the function **model_fit**.\n",
    "- **x**: this is the data that we want to make predictions for. It should be organized in a pandas dataset and it should have the same columns as the data that was used to fit the model.\n",
    "\n",
    "Given the fitted model and a pandas dataframe, this function creates a copy of the dataframe and adds to it a column named `'y'` in which all entries are `np.nan`. Then, for every leaf, the function creates a mask using its **query** and then it uses the mask to enter in the `'y'` column the value of the leaf's **prediction**. At the end, the function returns the values of the `'y'` column, i.e. the predictions for the given data, organized in a NumPy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c5819-b2bc-48e9-8d92-7ca0d4a03be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7177d-dcb7-4e32-bf1c-3985cad09581",
   "metadata": {
    "tags": []
   },
   "source": [
    "We should now test our functions in order to compare them to scikit-learn's corresponding functions. First, let's train decision trees using `model_fit` on the iris dataset and the breast cancer dataset. Since we want to compare these model's to scikit-learn's models that we previously fit, we will set all parameters to scikit-learn's default values. After fitting the decision tree, we will make predictions on the testing data using `model_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "650575a2-f2b1-4960-a84c-2ed80c09e2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 0.43599772453308105 seconds\n",
      "Iris prediction time: 0.059534311294555664 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "iris_hard_coded_tree = model_fit(X_train_iris, y_train_iris, np.inf, 2, 1)\n",
    "t_2 = time.time()\n",
    "iris_hard_coded_tree_predictions = model_predict(iris_hard_coded_tree, X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_hard_coded_tree_training_time = t_2 - t_1\n",
    "iris_hard_coded_tree_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_hard_coded_tree_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_hard_coded_tree_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6bfa02bb-8be3-4904-9fbf-386c73729554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scikit-Learn Tree</th>\n",
       "      <th>Hard-Coded Tree</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iris Train</th>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.423242</td>\n",
       "      <td>101.736776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris Predict</th>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>19.931096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Scikit-Learn Tree  Hard-Coded Tree      Ratios\n",
       "Iris Train             0.004160         0.423242  101.736776\n",
       "Iris Predict           0.001997         0.039793   19.931096"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_tree_results = pd.DataFrame({'Scikit-Learn Tree':[iris_tree_training_time,iris_tree_prediction_time],\n",
    "                                  'Hard-Coded Tree':[iris_hard_coded_tree_training_time,iris_hard_coded_tree_prediction_time]},\n",
    "                                 index=['Iris Train','Iris Predict'])\n",
    "iris_tree_results['Ratios'] = iris_tree_results['Hard-Coded Tree']/iris_tree_results['Scikit-Learn Tree']\n",
    "iris_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a59d7-f8a5-4e90-b016-7e47329d2a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The above table shows that although our hard-coded decision tree is fit in less than 1 second, its training time is roughly 100 times larger than the one for scikit-learns tree. Also, the prediction time of our hard-coded model is roughly 20 times larger than scikit-learn's tree. \n",
    "\n",
    "Let's now do the same test for the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "1233c03e-53fb-4bef-b99d-53db2529de04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 57.664549112319946 seconds\n",
      "Cancer prediction time: 0.11649060249328613 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "cancer_hard_coded_tree = model_fit(X_train_cancer, y_train_cancer, np.inf, 2, 1)\n",
    "t_2 = time.time()\n",
    "cancer_hard_coded_tree_predictions = model_predict(cancer_hard_coded_tree, X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_hard_coded_tree_training_time = t_2 - t_1\n",
    "cancer_hard_coded_tree_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_hard_coded_tree_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_hard_coded_tree_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5144251f-5e38-4a03-ba95-9c1024507e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scikit-Learn Tree</th>\n",
       "      <th>Hard-Coded Tree</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cancer Train</th>\n",
       "      <td>0.008660</td>\n",
       "      <td>58.161451</td>\n",
       "      <td>6715.857422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cancer Predict</th>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.122673</td>\n",
       "      <td>40.880820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Scikit-Learn Tree  Hard-Coded Tree       Ratios\n",
       "Cancer Train             0.008660        58.161451  6715.857422\n",
       "Cancer Predict           0.003001         0.122673    40.880820"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_tree_results = pd.DataFrame({'Scikit-Learn Tree':[cancer_tree_training_time,cancer_tree_prediction_time],\n",
    "                                  'Hard-Coded Tree':[cancer_hard_coded_tree_training_time,cancer_hard_coded_tree_prediction_time]},\n",
    "                                 index=['Cancer Train','Cancer Predict'])\n",
    "cancer_tree_results['Ratios'] = cancer_tree_results['Hard-Coded Tree']/cancer_tree_results['Scikit-Learn Tree']\n",
    "cancer_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb328f-9704-4c44-96ca-f60c8cd4fd26",
   "metadata": {},
   "source": [
    "The ratio of the prediction times is still of order $10^1$, but the ratio of the fitting times is now of order $10^3$. This means that our hard-coded decision tree is significantly slower on this dataset. We believe that this is because our algorithm iterates through all unique values of each column to determine the best cut at each step. Since the breat cancer dataset has many more columns, more rows, and is made of of floats only (except for the target), our algorithm has to iterate to a large number of unique values to fit the decision tree. When we will train a random forest on this dataset, we expect that the fitting time will be very large. However, it should not simply be (**cancer_hard_coded_tree_training_time**)*(**number of trees**) because as we will see, the trees in a random forest are trained on a bootstrapped dataset (which are likely to have several identical datapoints), and they only use a small subset of all columns at each step. Because of this, the unique values the algorithm will have to iterate through should be significantly smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a73ce-acfa-4e57-8579-cee9a5fe7b41",
   "metadata": {},
   "source": [
    "Let's not compare the predictions produced by scikit-learn's `DecisionTreeClassifier()` and our hard-coded functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d8a74d92-a202-41b9-b6ff-3d88f91cb592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Same Predictions Percentage: 100.0%\n",
      "Iris Different Predictions Percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "predictions_comparison = iris_hard_coded_tree_predictions == y_pred_iris\n",
    "\n",
    "same_count = np.count_nonzero(predictions_comparison)\n",
    "different_count = np.size(predictions_comparison) - same_count\n",
    "\n",
    "print(f'Iris Same Predictions Percentage: {round((same_count/predictions_comparison.size) * 100, 2)}%')\n",
    "print(f'Iris Different Predictions Percentage: {round((different_count/predictions_comparison.size) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1e54f41d-9f2d-4fcf-9e00-ec499522acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Scikit-Learn Tree Accuracy: 96.67%\n",
      "Iris Hard-Coded Tree Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris_tree_accuracy = accuracy_score(y_test_iris, y_pred_iris)\n",
    "iris_hard_coded_tree_accuracy = accuracy_score(y_test_iris, iris_hard_coded_tree_predictions)\n",
    "\n",
    "print(f'Iris Scikit-Learn Tree Accuracy: {round((iris_tree_accuracy) * 100, 2)}%')\n",
    "print(f'Iris Hard-Coded Tree Accuracy: {round((iris_hard_coded_tree_accuracy) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c250f15b-9632-47d6-9d26-6ecf94223e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  3  1]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test_iris, iris_hard_coded_tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece2ccf-b5ce-4c7a-86fe-ad53545714ea",
   "metadata": {},
   "source": [
    "Thus, both scikit-learn's `DecisionTreeClassifier()` and our hard-coded model made the same predictions and misclassified the same datapoint.  \n",
    "\n",
    "Let's now do the same comparisons for the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d79bee59-0847-4c33-b2ef-f05701cb5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer Same Predictions Percentage: 99.12%\n",
      "Cancer Different Predictions Percentage: 0.88%\n"
     ]
    }
   ],
   "source": [
    "predictions_comparison = cancer_hard_coded_tree_predictions == y_pred_cancer\n",
    "\n",
    "same_count = np.count_nonzero(predictions_comparison)\n",
    "different_count = np.size(predictions_comparison) - same_count\n",
    "\n",
    "print(f'Cancer Same Predictions Percentage: {round((same_count/predictions_comparison.size) * 100, 2)}%')\n",
    "print(f'Cancer Different Predictions Percentage: {round((different_count/predictions_comparison.size) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "5aae0acb-ebe0-47d5-848f-3853040dd1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer Scikit-Learn Tree Accuracy: 92.98%\n",
      "Cancer Hard-Coded Tree Accuracy: 92.11%\n"
     ]
    }
   ],
   "source": [
    "cancer_tree_accuracy = accuracy_score(y_test_cancer, y_pred_cancer)\n",
    "cancer_hard_coded_tree_accuracy = accuracy_score(y_test_cancer, cancer_hard_coded_tree_predictions)\n",
    "\n",
    "print(f'Cancer Scikit-Learn Tree Accuracy: {round((cancer_tree_accuracy) * 100, 2)}%')\n",
    "print(f'Cancer Hard-Coded Tree Accuracy: {round((cancer_hard_coded_tree_accuracy) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "fc28ca64-c038-47a2-94df-d9f9d8dd2c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  5]\n",
      " [ 4 69]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_cancer, cancer_hard_coded_tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc11ce-917c-4592-a394-1183ceb5af24",
   "metadata": {},
   "source": [
    "There are more misclassified datapoints now but the accuracy of our hard-coded trees is still comparable to scikit-learn's `DecisionTreeClassifier()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75788eb-e9a3-4af1-86ae-fcb14b71edd6",
   "metadata": {},
   "source": [
    "# Building the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fe9c0-5b13-4583-8ebd-341b7e498d91",
   "metadata": {},
   "source": [
    "Now that we have a function to fit a decision tree classifier and a function to make predictions using the fitted model, we can build the decision tree classifier. First, let's briefly test scikit-learn's random forest classifier to check its training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e46bd0e4-f289-48eb-b4f3-da322d73ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 0.14788818359375 seconds\n",
      "Iris prediction time: 0.016991138458251953 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_rf_iris = RandomForestClassifier( max_features=3)\n",
    "\n",
    "t_1 = time.time()\n",
    "clf_rf_iris.fit(X_train_iris, y_train_iris)\n",
    "t_2 = time.time()\n",
    "y_pred_iris_rf = clf_rf_iris.predict(X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_random_forest_training_time = t_2 - t_1\n",
    "iris_random_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_random_forest_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_random_forest_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d6e56106-5f0a-401e-86fd-eb19484fe8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 0.2697596549987793 seconds\n",
      "Cancer prediction time: 0.01598501205444336 seconds\n"
     ]
    }
   ],
   "source": [
    "clf_rf_cancer = RandomForestClassifier()\n",
    "\n",
    "t_1 = time.time()\n",
    "clf_rf_cancer.fit(X_train_cancer, y_train_cancer)\n",
    "t_2 = time.time()\n",
    "y_pred_cancer_rf = clf_rf_cancer.predict(X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_random_forest_training_time = t_2 - t_1\n",
    "cancer_random_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_rf_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_rf_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b68e89-2ec2-419b-b4a1-518e2211a6d4",
   "metadata": {},
   "source": [
    "Comparing these with the training and prediction times for the decision trees, we see that random forests take significantly more time to be trained and to make predictions. Specifically, the training times for random forests are 2 orders of magnitude larger than for decision trees, while the prediction time for random forests is an order of magnitude larger than for decision trees. The table below summarizes these observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fb93a231-e593-49b2-b8dd-081cdd9f2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris_Training_Time</th>\n",
       "      <th>Cancer_Training_Time</th>\n",
       "      <th>Iris_Prediction_Time</th>\n",
       "      <th>Cancer_Prediction_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.003001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.201620</td>\n",
       "      <td>0.269760</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.015985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF/Tree</th>\n",
       "      <td>48.464325</td>\n",
       "      <td>31.148937</td>\n",
       "      <td>5.784571</td>\n",
       "      <td>5.327030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Iris_Training_Time  Cancer_Training_Time  Iris_Prediction_Time  \\\n",
       "Tree               0.004160              0.008660              0.001997   \n",
       "RF                 0.201620              0.269760              0.011549   \n",
       "RF/Tree           48.464325             31.148937              5.784571   \n",
       "\n",
       "         Cancer_Prediction_Time  \n",
       "Tree                   0.003001  \n",
       "RF                     0.015985  \n",
       "RF/Tree                5.327030  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = pd.DataFrame({'Iris_Training_Time':[iris_tree_training_time,iris_rf_training_time,iris_rf_training_time/iris_tree_training_time],\n",
    "                      'Cancer_Training_Time':[cancer_tree_training_time,cancer_rf_training_time,cancer_rf_training_time/cancer_tree_training_time],\n",
    "                      'Iris_Prediction_Time':[iris_tree_prediction_time,iris_rf_prediction_time,iris_rf_prediction_time/iris_tree_prediction_time],\n",
    "                      'Cancer_Prediction_Time':[cancer_tree_prediction_time,cancer_rf_prediction_time,cancer_rf_prediction_time/cancer_tree_prediction_time]},\n",
    "                     index = ['Tree','RF','RF/Tree'])\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1891c-0cbe-4961-bc66-36b35676052a",
   "metadata": {},
   "source": [
    "Like before, to fit the model we need to provide a pandas data frame not containing the target, and a pandas series containing the target, while to make predictions we need to provide a new pandas data frame. The random forest classifier is based on decision trees, some some of the hyperparameters are the same as for the decision tree classifier. Speficically these are:\n",
    "- **criterion='gini'**\n",
    "- **max_depth=None**\n",
    "- **min_samples_split=2**\n",
    "- **min_samples_leaf=1**\n",
    "- **max_leaf_nodes=None**\n",
    "\n",
    "However, we now have **max_features='sqrt'**. This means that rather than using all features to find the best cut, only a randomly selected subset of all features is used *at each step*. For example, say that we have 9 features. Then, if **max_features='sqrt'**, only $\\sqrt9 = 3$ randomly selected features will be used to find the first best split, and 3 randomly selected features will be used to find the next best split, and so on until the tree is fit. \n",
    "\n",
    "Next, we have a few new hyperparameters of interest:\n",
    "- **n_estimators=100** indicates that the random forest will be made of 100 decision trees\n",
    "- **bootstrap=True** indicates that each decision tree is fit on a bootstrapped dataset \n",
    "- **n_jobs=None** indicates that no trees are fit in parallel\n",
    "- **random_state=None** indicates that no seed has been set; we will later set a seed so that the results of a random forest are reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc8a44-38e4-4531-a818-5fbdf13d5de2",
   "metadata": {},
   "source": [
    "Now that we have a better idea of what hyperparameters we need, we can start building our random forest classifier. First, we will need a modified version of the decision tree classifier we previously built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a3302f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_decision_tree_fit(x,y,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    df = x.copy()\n",
    "    # subset = (mask, prediction, depth, query)\n",
    "    potential_nodes = [(pd.Series([np.nan,np.nan]),y.value_counts().idxmax(), 0, 'True')]\n",
    "    leaves = []\n",
    "    columns = df.columns\n",
    "    while len(potential_nodes) != 0:\n",
    "        for subset in potential_nodes:\n",
    "            if subset[2]==0:\n",
    "                dropped_columns = np.random.choice(columns, df.shape[1]-max_features, replace='False')\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit(df.drop(dropped_columns,axis=1),y)\n",
    "                    mask_1 = df[cut_variable] < cut_value\n",
    "                    mask_2 = df[cut_variable] >= cut_value\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns = np.append(dropped_columns, cut_variable)\n",
    "                        if len(dropped_columns) == len(df.columns):\n",
    "                                return \"No branches found. Please, try to increase the value of min_samples_leaf.\"\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = 1\n",
    "                query_1 = '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                query_2 = '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            else:\n",
    "                dropped_columns = np.random.choice(columns, df.shape[1]-max_features, replace='False')\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit(df[subset[0]].drop(dropped_columns,axis=1),y[subset[0]]) \n",
    "                    mask_1 = (subset[0]) & (df[cut_variable] < cut_value)\n",
    "                    mask_2 = (subset[0]) & (df[cut_variable] >= cut_value)\n",
    "                    query_1 = subset[3] + '&' + '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                    query_2 = subset[3] + '&' + '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns = np.append(dropped_columns, cut_variable)\n",
    "                        if len(dropped_columns) == len(df.columns):\n",
    "                            leaves.append(subset)\n",
    "                            #potential_nodes.remove(subset)\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = subset[2]+1\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            # check if the new nodes are leaves \n",
    "            if subset_not_in_leaves(subset,leaves):\n",
    "                if node_1[2] == max_depth or df[mask_1].shape[0] <= min_samples_split or y[mask_1].unique().shape[0]==1:\n",
    "                    leaves.append(node_1)\n",
    "                else:\n",
    "                    potential_nodes.append(node_1)\n",
    "                if node_2[2] == max_depth or df[mask_2].shape[0] <= min_samples_split or y[mask_2].unique().shape[0]==1:\n",
    "                    leaves.append(node_2)\n",
    "                else:\n",
    "                    potential_nodes.append(node_2)\n",
    "            potential_nodes = remove_subset_from_nodes(subset,potential_nodes)\n",
    "    return restructure_leaves(leaves) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae88484-dd14-4746-bb6f-bdad14d83acd",
   "metadata": {},
   "source": [
    "This function is very similar to the one we coded for standard decision trees, but there are some crucial differences. First, the variable **x** can be any dataset, but we will use bootstrapped datasets with this function. Second, once a subset of **x** is selected, a subset of the columns of size `df.shape[1]-max_features` is *randomly* selected and dropped. In other words, the best split on the subset is performed using a randomly selected subset of the columns of size `max_features`. Next, we can finally code a function to train the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71f8b545-5837-4d1f-b904-abed0355a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(x,y,max_depth,min_samples_split,min_samples_leaf,n_estimators,max_features,random_seed):\n",
    "    np.random.seed(seed=random_seed)\n",
    "    results = []\n",
    "    for estimators in range(n_estimators):\n",
    "        bootstrap_x = x.sample(n=len(x), replace=True)\n",
    "        bootstrap_y = y[bootstrap_x.index]\n",
    "        # oob_x = x.drop(bootstrap_x.index)\n",
    "        tree = random_decision_tree_fit(bootstrap_x,bootstrap_y,max_depth,min_samples_split,min_samples_leaf,max_features)\n",
    "        results.append(tree)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1154ae3-c55e-4765-9f62-12611a0c4522",
   "metadata": {},
   "source": [
    "In this function, **x** and **y** are the training data and the corresponding training target, respectively. All the other variables are the same scikit-learn's `RandomForestClassifier()` that we discussed above. Here's how the function works:\n",
    "1. A random seed is set. The value of the random seed is a hyperparameter. \n",
    "2. A for loop is started. The number of iterations is equal to the hyperparameter value of **n_estimators**, which is the number of trees in the random forest.\n",
    "3. A bootstrapped sample is obtained using the original dataset **x**, and the corresponding **y** datapoints are selected. We also obtained the out-of-bag (oob) datapoints but we decided not to implement the oob score for now. We might implement it in the future. \n",
    "4. Using the boostrapped samples, a decision tree is fit using the modified decision tree function we discussed before.\n",
    "5. The leaves of the trained decision tree is stored in a list called `results`, which will be returned once the desired number of decision trees has been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d466104-3681-430f-960b-12af91c38815",
   "metadata": {},
   "source": [
    "Next, we need a function to make predictions using the trained random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7b38e6b2-4430-4caa-b823-f207c12ca5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_predict(forest, x):\n",
    "    df=x.copy()\n",
    "    #df['y']=np.nan\n",
    "    predictions_df = pd.DataFrame()\n",
    "    tree_number = 0\n",
    "    for tree in forest:\n",
    "        tree_number += 1 \n",
    "        predictions_df[f'tree_{tree_number}'] = model_predict(tree, df)\n",
    "    final_predictions = predictions_df.mode(axis=1)[0]\n",
    "    return final_predictions.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ed6ab-32a1-4eae-a083-9dcbf9fa10dc",
   "metadata": {},
   "source": [
    "The variables of this function are:\n",
    "- **forest**: the trained random forest produced by `train_random_forest`\n",
    "- **x**: the new data stored in a pandas dataframe that we want to make predictions on \n",
    "\n",
    "This is how the function works:\n",
    "1. Create an empty dataframe named `predictions_df` and set the variable `tree_number = 0`\n",
    "2. Start a `for` loop to iterate through all the trees in the trained random forest\n",
    "3. At each step, add 1 to `tree_number` and add a column to `predictions_df` with the predictions produced by the current tree in the `for` loop. To obtain the predictions, the function uses `model_predict`, which is the same function we coded to make predictions for a single decision tree. \n",
    "4. The function obtains the median, i.e., the most frequent value, for each row in `predictions_df`. This means that for each data point, the function selects the most popular prediction among all trees in the random forest. Notice that the function selected only the first element of `mode()` because we are not interested in the cases in which there are multiple modes.\n",
    "5. The function returns the predictions in a NumPy array.\n",
    "\n",
    "Notice that `predictions_df` is a pandas dataframe, not a NumPy array. We decided to use a pandas dataframe because it is easy to obtain the mode of its rows. If we wanted to obtain the mode of the rows of a NumPy array, we would either have to use ` scipy.stats.mode()`, which would require us to import an additional library, or we would have to write a function to obtain the mode. Since neither of these options is significantly advantageous, we opted for the easier option of using pandas. \n",
    "\n",
    "In addition, it could be interesting to return `predictions_df` in order to get a better idea of how the trees make different predictions. If one wishes to obtain this, the last line can be modified to `return final_predictions.values, predictions_df` so that `random_forest_predict(forest, x)[1]` returns `predictions_df`. In the simple case of a forest made of 3 trees and a dataset with only 3 data points, `predictions_df` would look like\n",
    "\n",
    "|  | **tree_1** | **tree_2** | **tree_3** |\n",
    "|--------------|--------------|--------------|--------------|\n",
    "| **0** | prediction of tree_1 for data point 1 | prediction of tree_2 for data point 1 | prediction of tree_3 for data point 1 |\n",
    "| **1** | prediction of tree_1 for data point 2 | prediction of tree_2 for data point 2 | prediction of tree_3 for data point 2 |\n",
    "| **2** | prediction of tree_1 for data point 3 | prediction of tree_2 for data point 3 | prediction of tree_3 for data point 3 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489df1b4-df25-49e0-ba4b-e6dc7b448ae5",
   "metadata": {},
   "source": [
    "# Testing the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c6a4d-d2b3-49bd-81c4-8faf92e85960",
   "metadata": {},
   "source": [
    "Let's not test our functions. In addition to testing the random forest, we should also test how long the algorithm takes to train one single tree. We are interested in this because we would like to confirm that training a decision tree on a bootstrapped dataset and only using a subset of the columns is indeed faster than training a standard decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1ba8549f-cf21-4d28-b635-a311474bdf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 0.14216136932373047 seconds\n",
      "Iris prediction time: 0.017999649047851562 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "iris_hard_coded_single_tree_forest = train_random_forest(X_train_iris, y_train_iris, np.inf, 2, 1, 1, 2, 42)\n",
    "t_2 = time.time()\n",
    "iris_hard_coded_single_tree_forest_predictions = random_forest_predict(iris_hard_coded_single_tree_forest, X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_hard_coded_single_tree_forest_training_time = t_2 - t_1\n",
    "iris_hard_coded_single_tree_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_hard_coded_single_tree_forest_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_hard_coded_single_tree_forest_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "3737b937-a3df-4f51-bd66-160ad15345b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hard-Coded Tree</th>\n",
       "      <th>Hard-Coded Signle Tree Forest</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iris Train</th>\n",
       "      <td>0.435998</td>\n",
       "      <td>0.142161</td>\n",
       "      <td>3.066921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris Predict</th>\n",
       "      <td>0.059534</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>3.307526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Hard-Coded Tree  Hard-Coded Signle Tree Forest    Ratios\n",
       "Iris Train           0.435998                       0.142161  3.066921\n",
       "Iris Predict         0.059534                       0.018000  3.307526"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_single_tree_forest_results = pd.DataFrame({'Hard-Coded Tree':[iris_hard_coded_tree_training_time,iris_hard_coded_tree_prediction_time],\n",
    "                                                'Hard-Coded Signle Tree Forest':[iris_hard_coded_single_tree_forest_training_time,iris_hard_coded_single_tree_forest_prediction_time]},\n",
    "                                 index=['Iris Train','Iris Predict'])\n",
    "iris_single_tree_forest_results['Ratios'] = iris_single_tree_forest_results['Hard-Coded Tree']/iris_single_tree_forest_results['Hard-Coded Signle Tree Forest']\n",
    "iris_single_tree_forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03b7a5-5424-4e12-8129-08cc8d23fda8",
   "metadata": {},
   "source": [
    "As expected, training a single tree random forest is faster than training a full decision tree. In this case, the difference seems not to be too dramatic. Let's check whether this is also the case for the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ac82a2a9-ca99-4a95-93eb-5295796c7e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 9.463565826416016 seconds\n",
      "Cancer prediction time: 0.06695413589477539 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "cancer_hard_coded_single_tree_forest = train_random_forest(X_train_cancer, y_train_cancer, np.inf, 2, 1, 1, 6, 42)\n",
    "t_2 = time.time()\n",
    "cancer_hard_coded_single_tree_forest_predictions = random_forest_predict(cancer_hard_coded_single_tree_forest, X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_hard_coded_single_tree_forest_training_time = t_2 - t_1\n",
    "cancer_hard_coded_single_tree_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_hard_coded_single_tree_forest_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_hard_coded_single_tree_forest_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "60d149f2-a859-49eb-a035-9d7d0da33ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hard-Coded Tree</th>\n",
       "      <th>Hard-Coded Single Tree Forest</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cancer Train</th>\n",
       "      <td>57.664549</td>\n",
       "      <td>9.463566</td>\n",
       "      <td>6.093321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cancer Predict</th>\n",
       "      <td>0.116491</td>\n",
       "      <td>0.066954</td>\n",
       "      <td>1.739857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Hard-Coded Tree  Hard-Coded Single Tree Forest    Ratios\n",
       "Cancer Train          57.664549                       9.463566  6.093321\n",
       "Cancer Predict         0.116491                       0.066954  1.739857"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_single_tree_forest_results = pd.DataFrame({'Hard-Coded Tree':[cancer_hard_coded_tree_training_time,cancer_hard_coded_tree_prediction_time],\n",
    "                                                'Hard-Coded Single Tree Forest':[cancer_hard_coded_single_tree_forest_training_time,cancer_hard_coded_single_tree_forest_prediction_time]},\n",
    "                                 index=['Cancer Train','Cancer Predict'])\n",
    "cancer_single_tree_forest_results['Ratios'] = cancer_single_tree_forest_results['Hard-Coded Tree']/cancer_single_tree_forest_results['Hard-Coded Single Tree Forest']\n",
    "cancer_single_tree_forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda04af8-7ab5-4d22-be98-cf1e631eccb7",
   "metadata": {},
   "source": [
    "Here the ratios are higher than before. Specifically, it is reassuring that one tree in the random forest is trained in roughly 10 seconds rather than 1 minute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6502d-130f-4dcc-b044-9888b6b53cf7",
   "metadata": {},
   "source": [
    "Let's now compare our hard-coded random forest to scikit-learn's `RandomForestClassifier()`. Like before, we will use the default values of the hyperparameters, meaning that we will train 100 trees and that we will use the square root of the number of columns. Notice that we actually had to set the number of features used to 3 (rather than $\\sqrt(4)=2$) because we were getting an error. This should be further investigated in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "8f2c9059-6bf5-4706-adc4-dc51837ed64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 21.370934009552002 seconds\n",
      "Iris prediction time: 2.8845767974853516 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "iris_hard_coded_random_forest = train_random_forest(X_train_iris, y_train_iris, np.inf, 2, 1, 100, 3, 42)\n",
    "t_2 = time.time()\n",
    "iris_hard_coded_random_forest_predictions = random_forest_predict(iris_hard_coded_random_forest, X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_hard_coded_random_forest_training_time = t_2 - t_1\n",
    "iris_hard_coded_random_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_hard_coded_random_forest_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_hard_coded_random_forest_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2e7af224-adab-495e-bcd9-161fe620a9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scikit-Learn Forest</th>\n",
       "      <th>Hard-Coded Forest</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iris Train</th>\n",
       "      <td>0.147888</td>\n",
       "      <td>21.370934</td>\n",
       "      <td>144.507380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris Predict</th>\n",
       "      <td>0.016991</td>\n",
       "      <td>2.884577</td>\n",
       "      <td>169.769483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Scikit-Learn Forest  Hard-Coded Forest      Ratios\n",
       "Iris Train               0.147888          21.370934  144.507380\n",
       "Iris Predict             0.016991           2.884577  169.769483"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_forest_results = pd.DataFrame({'Scikit-Learn Forest':[iris_random_forest_training_time,iris_random_forest_prediction_time],\n",
    "                                  'Hard-Coded Forest':[iris_hard_coded_random_forest_training_time,iris_hard_coded_random_forest_prediction_time]},\n",
    "                                 index=['Iris Train','Iris Predict'])\n",
    "iris_forest_results['Ratios'] = iris_forest_results['Hard-Coded Forest']/iris_forest_results['Scikit-Learn Forest']\n",
    "iris_forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf4079-b6bb-4415-ad84-b0a3dcfacaa6",
   "metadata": {},
   "source": [
    "From the above table we see that both ratios are of order $10^2$, meaning that the hard-coded functions are significantly slower than scikit-learn's `RandomForestClassifier()`.\n",
    "\n",
    "Next, let's check how our functions perform on the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3c241225-8665-4f21-8267-9c3bd8d1dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 1412.0974168777466 seconds\n",
      "Cancer prediction time: 7.38637375831604 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "cancer_hard_coded_random_forest = train_random_forest(X_train_cancer, y_train_cancer, np.inf, 2, 1, 100, 6, 42)\n",
    "t_2 = time.time()\n",
    "cancer_hard_coded_random_forest_predictions = random_forest_predict(cancer_hard_coded_random_forest, X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_hard_coded_random_forest_training_time = t_2 - t_1\n",
    "cancer_hard_coded_random_forest_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_hard_coded_random_forest_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_hard_coded_random_forest_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "34e90acf-6577-4af5-ba60-860d45429d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scikit-Learn Forest</th>\n",
       "      <th>Hard-Coded Forest</th>\n",
       "      <th>Ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cancer Train</th>\n",
       "      <td>0.191165</td>\n",
       "      <td>1412.097417</td>\n",
       "      <td>7386.809284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cancer Predict</th>\n",
       "      <td>0.011077</td>\n",
       "      <td>7.386374</td>\n",
       "      <td>666.839514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Scikit-Learn Forest  Hard-Coded Forest       Ratios\n",
       "Cancer Train               0.191165        1412.097417  7386.809284\n",
       "Cancer Predict             0.011077           7.386374   666.839514"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_forest_results = pd.DataFrame({'Scikit-Learn Forest':[cancer_random_forest_training_time,cancer_random_forest_prediction_time],\n",
    "                                  'Hard-Coded Forest':[cancer_hard_coded_random_forest_training_time,cancer_hard_coded_random_forest_prediction_time]},\n",
    "                                 index=['Cancer Train','Cancer Predict'])\n",
    "cancer_forest_results['Ratios'] = cancer_forest_results['Hard-Coded Forest']/cancer_forest_results['Scikit-Learn Forest']\n",
    "cancer_forest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ac88e-0853-4e92-8c6c-e6ae61916c6e",
   "metadata": {},
   "source": [
    "Unfortunately, the training time on this dataset is not great. Specifically, while scikit-learn's `RandomForestClassifier()` can train 100 trees is less than a second, our hard-coded model achieves the same in around 20 minutes, giving us a ratio of order $10^3$. Like with the iris dataset, the prediction ratio is still of order $10^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8485beb-801b-4e6d-b459-ee55b9a3c311",
   "metadata": {},
   "source": [
    "Let's now compare the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5f238942-5559-404f-a189-d7048db86361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Same Predictions Percentage: 100.0%\n",
      "Iris Different Predictions Percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "predictions_comparison = iris_hard_coded_random_forest_predictions == y_pred_iris_rf\n",
    "\n",
    "same_count = np.count_nonzero(predictions_comparison)\n",
    "different_count = np.size(predictions_comparison) - same_count\n",
    "\n",
    "print(f'Iris Same Predictions Percentage: {round((same_count/predictions_comparison.size) * 100, 2)}%')\n",
    "print(f'Iris Different Predictions Percentage: {round((different_count/predictions_comparison.size) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a342433d-78b4-4498-b164-43f344559c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Scikit-Learn Tree Accuracy: 100.0%\n",
      "Iris Hard-Coded Tree Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "iris_random_forest_accuracy = accuracy_score(y_test_iris, y_pred_iris_rf)\n",
    "iris_hard_coded_random_forest_accuracy = accuracy_score(y_test_iris, iris_hard_coded_random_forest_predictions)\n",
    "\n",
    "print(f'Iris Scikit-Learn Tree Accuracy: {round((iris_random_forest_accuracy) * 100, 2)}%')\n",
    "print(f'Iris Hard-Coded Tree Accuracy: {round((iris_hard_coded_random_forest_accuracy) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323fc5d-8946-4f99-af4a-4129f327455e",
   "metadata": {},
   "source": [
    "So, both models produce perfect predictions. This is not too surprising since even the single decision trees misclassified only one datapoit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "52e377f4-71aa-4bef-aad6-bf85fb1bef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer Same Predictions Percentage: 100.0%\n",
      "Cancer Different Predictions Percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "predictions_comparison = cancer_hard_coded_random_forest_predictions == y_pred_cancer_rf\n",
    "\n",
    "same_count = np.count_nonzero(predictions_comparison)\n",
    "different_count = np.size(predictions_comparison) - same_count\n",
    "\n",
    "print(f'Cancer Same Predictions Percentage: {round((same_count/predictions_comparison.size) * 100, 2)}%')\n",
    "print(f'Cancer Different Predictions Percentage: {round((different_count/predictions_comparison.size) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ff67c156-98ae-4f6a-9dc4-bef7884a8b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer Scikit-Learn Forest Accuracy: 95.61%\n",
      "Cancer Hard-Coded Forest Accuracy: 95.61%\n"
     ]
    }
   ],
   "source": [
    "cancer_random_forest_accuracy = accuracy_score(y_test_cancer, y_pred_cancer_rf)\n",
    "cancer_hard_coded_random_forest_accuracy = accuracy_score(y_test_cancer, cancer_hard_coded_random_forest_predictions)\n",
    "\n",
    "print(f'Cancer Scikit-Learn Forest Accuracy: {round((cancer_random_forest_accuracy) * 100, 2)}%')\n",
    "print(f'Cancer Hard-Coded Forest Accuracy: {round((cancer_hard_coded_random_forest_accuracy) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "3421cda3-ae9f-4406-b1e0-5e037cc1490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  2]\n",
      " [ 3 70]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_cancer, cancer_hard_coded_random_forest_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388d79b-a9ed-468c-82bd-51a14e3473e4",
   "metadata": {},
   "source": [
    "The above results indicate that the two models produce the same predictions. Thus, we can conclude that our hard-coded functions are equivalent to scikit-learn's `RandomForestClassifier()`, except for the fact that scikit-learn's `RandomForestClassifier()` is *way faster* (and has more hyperparameters). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7d0bc-0f82-454e-9809-2828ba1d79ce",
   "metadata": {},
   "source": [
    "# Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e5dd6-e177-4519-9f58-ada5db99e268",
   "metadata": {},
   "source": [
    "A lot could be done to improve the training time of our random forest classifier. Clearly, the main issue of our random forest is that it takes a fairly long time to train each tree, and so, to improve the training speed of `train_random_forest`, we need to improve the training speed of `random_decision_tree_fit`. Unfortunately, we couldn't find much about this in the machine learning textbooks that we cited below. However, we managed to find some interesting ideas at the following link: https://stats.stackexchange.com/questions/105487/the-efficiency-of-decision-tree. Some of the main things that we could do are:\n",
    "- **Vectorizing** everything that still needs to be vectorized; specifically, many our functions contain several loops that could be rewritten as vector or matrix operations\n",
    "- Computing some operations in **parallel**: for example, we could fit more than one tree at the same time, or we could try to find the best split for more than one subset of the data at the same time\n",
    "- Searching for a **good split** rather than the best split: this would allow us to obtain best best split considering only a randomly selected subset of each column; this would lead to a split that is not guaranteed to be the best split, but it would allow the algorithm to ignore several values in each columns; while this might lead to imprecise decision trees, the errors that this might cause would be greatly mitigated in large random forests\n",
    "\n",
    "Rather than focusing on these, we would like to modify the algorithm that `random_decision_tree_fit` uses to find the best split. Specifically, for each column, `random_decision_tree_fit` finds the best split by iterating through each value and calculating the weighted Gini that splitting the data at the considered datapoint would produce. Rather than doing this, we will split the data into two bins (one of which is initially empty), and calcualte the weigted Gini that this gives. Then, we will move one datapoint at a time to the other bin, and at each step we calculate weighted Gini using the content of the bins rather than using the datapoints themselves. This has the advantage that rather than splitting the column every time we need to check a new potential best split, we split the column one time into two bins and then we move one datapoint at a time. We will try to apply this trick to columns containing continuous variables since those are the ones that are slowing down the training time the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "0f887a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_min_in_left(left):\n",
    "    # min_so_far = [min value, y value]\n",
    "    min_so_far = [np.inf, 0]\n",
    "    for i in left.keys():\n",
    "        if left[i].shape[0] != 0: \n",
    "            if left[i].iloc[0] < min_so_far[0]:\n",
    "                min_so_far = [left[i].iloc[0],i]\n",
    "    return min_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2bfbd768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gini_of_node(node):\n",
    "    probSquared = np.array([])\n",
    "    tot_length = 0\n",
    "    for i in node.keys():\n",
    "        tot_length += node[i].shape[0]\n",
    "    if tot_length == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        for i in node.keys():\n",
    "            prob = np.divide(node[i].shape[0],tot_length)\n",
    "            probSquared = np.append(probSquared,np.square(prob))\n",
    "        return 1-np.sum(probSquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8bd84dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_Gini(left, right):\n",
    "    total_length = 0\n",
    "    left_length = 0\n",
    "    right_length = 0\n",
    "    for i in left.keys():\n",
    "        total_length += left[i].shape[0]\n",
    "        left_length += left[i].shape[0]\n",
    "    for i in right.keys():\n",
    "        total_length += right[i].shape[0]\n",
    "        right_length += right[i].shape[0]\n",
    "    if left_length == 0:\n",
    "        weight_left = 0\n",
    "    else:\n",
    "        weight_left = np.divide(left_length,total_length)\n",
    "    if right_length == 0:\n",
    "        weight_right = 0\n",
    "    else:\n",
    "        weight_right = np.divide(right_length,total_length)\n",
    "    return weight_left*gini_of_node(left) + weight_right*gini_of_node(right)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "df5441b3-08dd-44c4-9bae-976f570c4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_for_continuous_variable(column,y):\n",
    "    left = {}\n",
    "    right = {} \n",
    "    for i in np.unique(y):\n",
    "        left[i] = column[y==i].sort_values()\n",
    "        right[i] = np.array([])\n",
    "    weigted_Gini_of_best_cut = 2\n",
    "    best_cut_value = 0\n",
    "    len_left = 0 \n",
    "    for i in left.keys():\n",
    "        len_left += left[i].shape[0]\n",
    "    len_right = 0\n",
    "    min_value_in_left = np.inf\n",
    "    while len_left != 0:\n",
    "        trial_cut, index_of_min = find_min_in_left(left)\n",
    "        current_Gini = weighted_Gini(left,right)\n",
    "        if current_Gini < weigted_Gini_of_best_cut:\n",
    "            best_cut_value = trial_cut\n",
    "            weigted_Gini_of_best_cut = current_Gini\n",
    "        left[index_of_min] = left[index_of_min].iloc[1:]\n",
    "        right[index_of_min] = np.append(right[index_of_min], trial_cut)\n",
    "        len_left -= 1\n",
    "        len_right += 1\n",
    "    return (best_cut_value,weigted_Gini_of_best_cut,column.name)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ac68-f754-4265-babd-0c466605f3cd",
   "metadata": {},
   "source": [
    "Here's how `get_split_for_continuous_variable` obtains the best split for a continuous variable:\n",
    "- Define two empty bins named left and right\n",
    "- For each value of the target y, create sorted subsets in each bin; the ones on the left contain all the data, the ones on the right are initially empty\n",
    "- Set the initial value of a few variables and obtain the size of the left bin\n",
    "- Start a while loop that stops when the size of the left bin is zero, meaning that all data has been passed to the right bin\n",
    "- At each iteration, try the minimum value in the left bin as the split value and obtain the weighted Gini, then move this point to the right bin; finding the minimum value is quick because the data is already sorted, and so the function just needs to check the first datapoint for each subset in the left bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7cc896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSubsetSplit_new(x,y):\n",
    "    results = np.array((np.nan,2,np.nan))\n",
    "    if np.unique(y).shape[0]<10:\n",
    "        for column in x.columns:\n",
    "            a=np.array(bestColumnSplit(x[str(column)],y))\n",
    "            results = np.vstack((results,a))\n",
    "    else:\n",
    "        for column in x.columns:\n",
    "            a=np.array(get_split_for_continuous_variable(x[str(column)],y))\n",
    "            results = np.vstack((results,a))\n",
    "    row_best_cut = results[:,1].argmin()\n",
    "    # return best cut value and column name\n",
    "    return (float(results[row_best_cut,0]),results[row_best_cut,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f4652076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_decision_tree_fit_new(x,y,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    df = x.copy()\n",
    "    # subset = (mask, prediction, depth, query)\n",
    "    potential_nodes = [(pd.Series([np.nan,np.nan]),y.value_counts().idxmax(), 0, 'True')]\n",
    "    leaves = []\n",
    "    columns = df.columns\n",
    "    while len(potential_nodes) != 0:\n",
    "        for subset in potential_nodes:\n",
    "            if subset[2]==0:\n",
    "                dropped_columns = np.random.choice(columns, df.shape[1]-max_features, replace='False')\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit_new(df.drop(dropped_columns,axis=1),y)\n",
    "                    mask_1 = df[cut_variable] < cut_value\n",
    "                    mask_2 = df[cut_variable] >= cut_value\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns = np.append(dropped_columns, cut_variable)\n",
    "                        if len(dropped_columns) == len(df.columns):\n",
    "                                return \"No branches found. Please, try to increase the value of min_samples_leaf.\"\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = 1\n",
    "                query_1 = '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                query_2 = '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            else:\n",
    "                dropped_columns = np.random.choice(columns, df.shape[1]-max_features, replace='False')\n",
    "                while True:\n",
    "                    cut_value, cut_variable = bestSubsetSplit(df[subset[0]].drop(dropped_columns,axis=1),y[subset[0]]) \n",
    "                    mask_1 = (subset[0]) & (df[cut_variable] < cut_value)\n",
    "                    mask_2 = (subset[0]) & (df[cut_variable] >= cut_value)\n",
    "                    query_1 = subset[3] + '&' + '(' + cut_variable + '<' + f'{cut_value}' + ')'\n",
    "                    query_2 = subset[3] + '&' + '(' + cut_variable + '>=' + f'{cut_value}' + ')'\n",
    "                    if (df[mask_1].shape[0]<min_samples_leaf) | (df[mask_2].shape[0]<min_samples_leaf):\n",
    "                        dropped_columns = np.append(dropped_columns, cut_variable)\n",
    "                        if len(dropped_columns) == len(df.columns):\n",
    "                            leaves.append(subset)\n",
    "                            #potential_nodes.remove(subset)\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                prediction_1 = y[mask_1].value_counts().idxmax()\n",
    "                prediction_2 = y[mask_2].value_counts().idxmax()\n",
    "                depth = subset[2]+1\n",
    "                node_1 = (mask_1, prediction_1, depth, query_1)\n",
    "                node_2 = (mask_2, prediction_2, depth, query_2)\n",
    "            # check if the new nodes are leaves \n",
    "            if subset_not_in_leaves(subset,leaves):\n",
    "                if node_1[2] == max_depth or df[mask_1].shape[0] <= min_samples_split or y[mask_1].unique().shape[0]==1:\n",
    "                    leaves.append(node_1)\n",
    "                else:\n",
    "                    potential_nodes.append(node_1)\n",
    "                if node_2[2] == max_depth or df[mask_2].shape[0] <= min_samples_split or y[mask_2].unique().shape[0]==1:\n",
    "                    leaves.append(node_2)\n",
    "                else:\n",
    "                    potential_nodes.append(node_2)\n",
    "            potential_nodes = remove_subset_from_nodes(subset,potential_nodes)\n",
    "    return restructure_leaves(leaves) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d007f50e-9e92-45f2-a0ee-38e38500daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_new(x,y,max_depth,min_samples_split,min_samples_leaf,n_estimators,max_features,random_seed):\n",
    "    np.random.seed(seed=random_seed)\n",
    "    results = []\n",
    "    for estimators in range(n_estimators):\n",
    "        bootstrap_x = x.sample(n=len(x), replace=True)\n",
    "        bootstrap_y = y[bootstrap_x.index]\n",
    "        # oob_x = x.drop(bootstrap_x.index)\n",
    "        tree = random_decision_tree_fit_new(bootstrap_x,bootstrap_y,max_depth,min_samples_split,min_samples_leaf,max_features)\n",
    "        results.append(tree)\n",
    "    return results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf476ae-f5fb-4e95-9f9e-283868d1d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris training time: 18.75035262107849 seconds\n",
      "Iris prediction time: 2.854611873626709 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "iris_hard_coded_random_forest_new = train_random_forest_new(X_train_iris, y_train_iris, np.inf, 2, 1, 100, 3, 42)\n",
    "t_2 = time.time()\n",
    "iris_hard_coded_random_forest_new_predictions = random_forest_predict(iris_hard_coded_random_forest_new, X_test_iris)\n",
    "t_3 = time.time()\n",
    "\n",
    "iris_hard_coded_random_forest_new_training_time = t_2 - t_1\n",
    "iris_hard_coded_random_forest_new_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Iris training time: {iris_hard_coded_random_forest_new_training_time} seconds')\n",
    "print(f'Iris prediction time: {iris_hard_coded_random_forest_new_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9697bef7-0139-4e5a-9b42-c80323e73dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer training time: 1277.8775141239166 seconds\n",
      "Cancer prediction time: 8.553619861602783 seconds\n"
     ]
    }
   ],
   "source": [
    "t_1 = time.time()\n",
    "cancer_hard_coded_random_forest_new = train_random_forest_new(X_train_cancer, y_train_cancer, np.inf, 2, 1, 100, 3, 42)\n",
    "t_2 = time.time()\n",
    "cancer_hard_coded_random_forest_new_predictions = random_forest_predict(cancer_hard_coded_random_forest_new, X_test_cancer)\n",
    "t_3 = time.time()\n",
    "\n",
    "cancer_hard_coded_random_forest_new_training_time = t_2 - t_1\n",
    "cancer_hard_coded_random_forest_new_prediction_time = t_3 - t_2 \n",
    "\n",
    "print(f'Cancer training time: {cancer_hard_coded_random_forest_new_training_time} seconds')\n",
    "print(f'Cancer prediction time: {cancer_hard_coded_random_forest_new_prediction_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015881-972b-4ef7-b134-65329f3caa49",
   "metadata": {},
   "source": [
    "Unfortunately, we see that the training time has not significantly improved. The other ideas we mentioned above should speed us the training time much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8b390813-8a29-4a13-a8e7-176933e23fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Predictions = 113\n",
      "Different Predictions = 1\n",
      "Cancer Hard-Coded Forest New Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "cancer_comparison = cancer_hard_coded_random_forest_new_predictions==cancer_hard_coded_random_forest_predictions\n",
    "same = np.count_nonzero(cancer_comparison)\n",
    "different = cancer_comparison.size - same\n",
    "print('Same Predictions =',same)\n",
    "print('Different Predictions =',different)\n",
    "\n",
    "cancer_hard_coded_random_forest_new_accuracy = accuracy_score(y_test_cancer, cancer_hard_coded_random_forest_new_predictions)\n",
    "print(f'Cancer Hard-Coded Forest New Accuracy: {round((cancer_hard_coded_random_forest_new_accuracy) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c9a1b-9626-4e82-8209-ea0d699cddee",
   "metadata": {},
   "source": [
    "So, the predictions are almost identical and the accuracy has slightly improved. These changes are not significant and these results are consistent with what we previously obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ae984-60a9-49e6-830e-55e691fa04f3",
   "metadata": {},
   "source": [
    "We conclude by visualizing the training times for all our models. Since these are extremely different, we will plot them on a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "c73854cd-e02b-47a7-a56c-29cfb1698a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = pd.DataFrame(columns = ['Name','Training Time'])\n",
    "\n",
    "training_times = training_times.append({'Name': 'Sklearn Tree Iris',\n",
    "                                       'Training Time':iris_tree_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'Sklearn Tree Cancer',\n",
    "                                       'Training Time':cancer_tree_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD Tree Iris',\n",
    "                                       'Training Time':iris_hard_coded_tree_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD Tree Cancer',\n",
    "                                       'Training Time':cancer_hard_coded_tree_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'SKlearn RF Iris',\n",
    "                                       'Training Time':iris_random_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'SKlearn RF Cancer',\n",
    "                                       'Training Time':cancer_random_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD 1 Tree RF Iris',\n",
    "                                       'Training Time':iris_hard_coded_single_tree_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD 1 Tree RF Cancer',\n",
    "                                       'Training Time':cancer_hard_coded_single_tree_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD RF Iris',\n",
    "                                       'Training Time':iris_hard_coded_random_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'HD RF Cancer',\n",
    "                                       'Training Time':cancer_hard_coded_random_forest_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'Improved HD RF Iris',\n",
    "                                       'Training Time':iris_hard_coded_random_forest_new_training_time}, ignore_index=True)\n",
    "training_times = training_times.append({'Name': 'Improved HD RF Cancer',\n",
    "                                       'Training Time':cancer_hard_coded_random_forest_new_training_time}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c0b84374-798b-4c79-b7e0-fb33a2e42658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAHVCAYAAACqkWxBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABtXElEQVR4nO3deXhdVfX/8fdK0yZpOjMUKEMZyiRFhTKJgCBDoSgqg4wyI4MICCg4AKIgoIgDMqggjgzyVUFQFASL8AOkIMgsg0UKmgJt6JS0DVm/P9a+zeklaQM02SfN5/U8eZp77m2y7sm556yzh7XN3RERERGRcqnJHYCIiIiIvJWSNBEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpIlIKZjbVzHbOHUc1M9vOzJ5ZwvNjzczNrLY348ohvc/1csch0l8oSRORSoLUYmZzzGymmd1qZmss498xzMy+Y2b/Sb/nufR4xWX5ewq/769mdtS7/Tnu/jd336Dwc99VMmlm15jZ199tXEv5HSPM7Goz+5+ZzTazf5nZF3ryd4rIsqckTUQqPuLuQ4BVgSbg++/kh3TWomRmg4C/AO8BJgLDgA8ArwNbvtOAu/j9Zmb9/dx2CTAE2AgYDnwUeD5rRCLytvX3E5mIVHH3VuBGYOPKNjObZGb/MLNZZvaSmZ1TeK7S3Xekmf0HuLOTH/spYE3g4+7+pLu3u/t0d/+au/+h8Lr3mdk/zewNM7vezOrT7xhpZreY2auppe8WM1u9EMNfzew8M7sXmAf8HNgOuDS12l1aHZCZ/dTMTk3fj0nv4fj0eD0zm5ESvg+Z2bS0/efpffw+/dzPF37kQamV8DUz+9Lb2OXFmI5OLYwzzOxmM1ut8NyuZvZM2jeXmdnkJbQUbgH8yt1npn39tLvfWPhZ7zGz29PvaTKzL6btW5rZfWbWbGb/NbNLU4LdWax1Zvat9J6bzOwKM2t4J+9bRDqnJE1EFmNmg4FPAvcXNs8lEq0RwCTgODP7WNV/3YFoudmtkx+7M3Cbu89Zyq/fj2hpWxvYFDgsba8BfgKsRSRJLUB14nUIcAwwNP2/vwGfcfch7v6ZTn7XZOBDhdhfSP8CbA/8zavWzXP3Q4D/kFod3f2iwtMfBDYAPgycZWYbLeW9LsbMdgK+QeyDVYEXgevScysSifOZwArAM0RLZFfuB84zs8PNbFzV7xkK3AHcBqwGrEe0cgK8CZwCrAhsk97L8V38jguB9YH3pZ8xBjiru+9XRJZOSZqIVPzOzJqBWcAuwDcrT7j7X939sdQq80/gWjoSmopz3H2uu7d08rNXAP7bjRi+5+6vuPsM4PdEAoC7v+7u/+fu89x9NnBeJ7//Gnd/wt3b3H1hN37XZGC71DW6PXARsG16bof0/NvxVXdvcfdHgUeB977N/38QcLW7P+zu84mEbBszGwvsATzh7r9x9zbge8D/lvCzTgR+CXwGeDK1zu2entsT+J+7X+zure4+290fAHD3h9z9/rQPpwJX8tb9jJkZcDRwirvPSH+T84H93+Z7FpElUJImIhUfc/cRQB1xcZ9sZqsAmNlWZnZX6m58AziWaG0pemkJP/t1onVoaYqJxzxiXBVmNtjMrjSzF81sFnA3MMLMBnTz97+Fuz8PzCESwe2AW4BXzGwD3lmS1mnsb8NqROtZJb45xH4bk557qfCcA9O6+kEpWTzf3TcnEuQbgF+b2ShgDboYn2Zm66eu5P+l/Xw+b/07A6wEDAYeSl2jzUTL3Epv4/2KyFIoSRORxbj7m+7+G6Lr64Np86+Am4E13H04cAVg1f91CT/2DmA3M2t8h2GdSnQlbuXuw4iWL6piqP79S4qnYjKwDzDI3V9Ojz8FjAQe6eL/dOfnvhOvEN25AKR9tQLwMtEKWRyDZ8XHS+LulWSrkehGfglYt4uXXw48DYxL+/mLvPXvDPAa0eX8Hncfkb6Gp4knIrKMKEkTkcWkwfJ7EYnKU2nzUGCGu7ea2ZbAgW/zx/6cSA7+z8w2NLMaM1vBzL5oZnt04/8PJZKC5tQadHY3/k8TsM5SXjOZaDW8Oz3+K9FVeI+7v/kufu7SDDCz+sLXICIRPtzM3mdmdURi9UDqdrwVGG9mH7OYPXsCsEpXP9zMvmJmW5jZoDT54iSgmRjLdguwipmdnAb/DzWzrdJ/HUp0d88xsw2B4zr7+e7eDvwIuMTMVk6/c4yZdTYeUUTeISVpIlLxezObQ1ykzwMOdfcn0nPHA+ea2WxicPgNb+cHpzFWOxOtNLen3/F3oivtgW78iO8ADUQLzv1E19rSfBfYx2I26Pe6eM1kIjGpJGn3EN14d3fxeojB/V9O3XyndSOOzpxBJJ2Vrzvd/S/AV4D/I1rO1iWN8XL314B9iXFzrxMzb6cA87v4+U5MtHiNaKHbBZjk7nPS+LFdgI8QXbTPAjum/3cakYDPJpKw65fwHr4APAfcn7pG7yBaO0VkGbGqyUsiIlJyabLDNOAgd78rdzwi0jPUkiYi0geY2W4WKwnU0TFW7P6l/DcR6cOUpImI9A3bELMyXyO6Kj/WRbkTEVlOqLtTREREpITUkiYiIiJSQkrSREREREqoNncAPWHFFVf0sWPH5g5DREREZKkeeuih19z9LSt2LJdJ2tixY5kyZUruMERERESWysxe7Gy7ujtFRERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJaQkTURERKSEanMHICIi0leNPePW3CEAMPWCSblDkB6gljQRERGRElJLmoiIiJSKWiiDWtJERERESqj0SZqZbWRmV5jZjWZ2XO54RERERHpDliTNzK42s+lm9njV9olm9oyZPWdmZwC4+1PufiywHzAhR7wiIiIivS1XS9o1wMTiBjMbAPwA2B3YGDjAzDZOz30UuAf4S++GKSIiIpJHliTN3e8GZlRt3hJ4zt1fcPcFwHXAXun1N7v7B4CDejdSERERkTzKNLtzDPBS4fE0YCsz+xDwCaAO+EN3flB7eztNTU0MHz6cuXPn0tbWxqhRo5gxYwb19fXU1NQwb948RowYwezZs2lvb2fEiBHMnDmThoYGAFpaWhg5ciTNzc3U1NQwdOhQmpubGTx4MO3t7bS2ti76mbW1tTQ2NvLGG2/Q2NhIW1sb8+fPX/T8wIEDaWhoYNasWQwdOpT58+ezYMGCRc8PGjSIuro6Zs+ezbBhw2hpaWHhwoWLnq+rq6O2tpa5c+fqPek96T3pPek9leg9HbjpcG57dg4Txw2haU4bL89qY7PV6nlgWgvjVhjEqIYBi55/edZCZrS8yfjR9dz7n3mMH13PsLqaRc9PnbmQ1rZ2NlypjslT57LlmAZqa4zJU+ey87pDeO71BQCst8Ig7nh+DjuMbaSt3fn7yy00NTUtV3+nMcNq2WFsI0+/Op/62hrGjhy4aD/Nmt/OY02tbLvmYB5ramVUwwDGDOt4fkbLmzz7+gK2Wr2Bh19pZcywWkYPqX1Hf6fW1tZeOfa6Yu7enbxnmTOzscAt7r5JerwvsJu7H5UeHwJs6e4nvt2fPWHCBJ8yZcqyDFdEROQtVCqiZ/S3/WpmD7n7W8bdl2l25zRgjcLj1YFXMsUiIiIiklWZkrQHgXFmtraZDQL2B27OHJOIiIhIFrlKcFwL3AdsYGbTzOxId28DPgP8CXgKuMHdn8gRn4iIiEhuWSYOuPsBXWz/A92cHCAiIiKyPCtTd6eIiIiIJErSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJaQkTURERKSElKSJiIiIlJCSNBEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpIiIiIiWkJE1ERESkhJSkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJaQkTURERKSElKSJiIiIlJCSNBEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpIiIiIiWkJE1ERESkhJSkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKaHSJ2lm9jEz+5GZ3WRmu+aOR0RERKQ3ZEnSzOxqM5tuZo9XbZ9oZs+Y2XNmdgaAu//O3Y8GDgM+mSFcERERkV6XqyXtGmBicYOZDQB+AOwObAwcYGYbF17y5fS8iIiIyHIvS5Lm7ncDM6o2bwk85+4vuPsC4DpgLwsXAn9094d7O1YRERGRHGpzB1AwBnip8HgasBVwIrAzMNzM1nP3K5b2g9rb22lqamL48OHMnTuXtrY2Ro0axYwZM6ivr6empoZ58+YxYsQIZs+eTXt7OyNGjGDmzJk0NDQA0NLSwsiRI2lubqampoahQ4fS3NzM4MGDaW9vp7W1ddHPrK2tpbGxkTfeeIPGxkba2tqYP3/+oucHDhxIQ0MDs2bNYujQocyfP58FCxYsen7QoEHU1dUxe/Zshg0bRktLCwsXLlz0fF1dHbW1tcydO1fvSe9J70nvSe+pRO/pwE2Hc9uzc5g4bghNc9p4eVYbm61WzwPTWhi3wiBGNQxY9PzLsxYyo+VNxo+u597/zGP86HqG1dUsen7qzIW0trWz4Up1TJ46ly3HNFBbY0yeOped1x3Cc68vAGC9FQZxx/Nz2GFsI23tzt9fbqGpqWm5+juNGVbLDmMbefrV+dTX1jB25MBF+2nW/HYea2pl2zUH81hTK6MaBjBmWMfzM1re5NnXF7DV6g08/EorY4bVMnpI7Tv6O7W2tvbKsdcVc/el5Tw9wszGAre4+ybp8b7Abu5+VHp8CLClu5/4dn/2hAkTfMqUKcsyXBERkbcYe8atuUMAYOoFk3KHsEz1t/1qZg+5+4Tq7WWa3TkNWKPweHXglUyxiIiIiGRVpiTtQWCcma1tZoOA/YGbM8ckIiIikkWuEhzXAvcBG5jZNDM70t3bgM8AfwKeAm5w9ydyxCciIiKSW5aJA+5+QBfb/wD8oZfDERERESmdMnV3ioiIiEiiJE1ERESkhJSkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJlWmBdZE+pb+tLSciIr1LLWkiIiIiJaQkTURERKSElKSJiIiIlJCSNBEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpIiIiIiWkJE1ERESkhJSkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREqoNncAIiIiRWPPuDV3CABMvWBS7hCkn1NLmoiIiEgJKUkTERERKSF1d4qIiPQD6kbue9SSJiIiIlJCStJERERESkhJmoiIiEgJdWtMmpmNBFYDWoCp7t7eo1GJiIiI9HNdJmlmNhw4ATgAGAS8CtQDo83sfuAyd7+rV6IUERER6WeW1JJ2I/AzYDt3by4+YWabA4eY2TruflUPxiciIiLSL3WZpLn7Lkt47iHgoR6JSERERESWPnHAwsFmdlZ6vKaZbdnzoYmIiIj0X92Z3XkZsA0xNg1gNvCDHotIRERERLo1u3Mrd9/MzP4B4O4zzWxQD8clIiIi0q91pyVtoZkNABzAzFYCeq0Eh5mtY2ZXmdmNvfU7RURERHLrTpL2PeC3wMpmdh5wD3D+u/mlZna1mU03s8ertk80s2fM7DkzOwPA3V9w9yPfze8TERER6WuW2t3p7r80s4eADwMGfMzdn3qXv/ca4FKixAcAqbXuB8AuwDTgQTO72d2ffJe/S0RERKTPWVIx21GFh9OBa4vPufuMd/pL3f1uMxtbtXlL4Dl3fyH9juuAvQAlaSIiItLvLKkl7SFiHJoVtlUeO7DOMo5lDPBS4fE0YCszWwE4D3i/mZ3p7t9Y2g9qb2+nqamJ4cOHM3fuXNra2hg1ahQzZsygvr6empoa5s2bx4gRI5g9ezbt7e2MGDGCmTNn0tDQAEBLSwsjR46kubmZmpoahg4dSnNzM4MHD6a9vZ3W1tZFP7O2tpbGxkbeeOMNGhsbaWtrY/78+YueHzhwIA0NDcyaNYuhQ4cyf/58FixYsOj5QYMGUVdXx+zZsxk2bBgtLS0sXLhw0fN1dXXU1tYyd+5cvacSvaeJ44YwqmEAtz07h4njhvDyrIXMaHmT8aPrufc/8xg/up5hdTWLnp86cyGtbe1suFIdk6fOZcsxDdTWGJOnzmXndYfw3OsLAFhvhUHc8fwcdhjbSFu78/eXW9hhbCNPvzqf+toaxo4cuOhnzpof71F/J72n5ek9Hbjp8Gyfp8eaWtl2zcE81tRKc3PzUt/TgZsOX/T/m+a08fKsNjZbrZ4HprUwboVBvXaOaGpqWurfafzoOkY1DGDMsI73PKPlTZ59fQFbrd7Aw6+0MmZYLaOH1Pboe5o+ffpSj70xw2q7/XfqyffU2traK5+nrpi7Ly3n6RGpJe0Wd98kPd4X2M3dj0qPDwG2dPcT3+7PnjBhgk+ZMmVZhivyFmPPuDV3CABMvWBS7hCkD+hLx6tiffsUa8/orfOrmT3k7hOqt7+dBdbHEWt3AtFluezCA6LlbI3C49WBV5bx7xARERHpE5aapJnZUcBJRNL0CLA1cB+w0zKO5UFgnJmtDbwM7A8cuIx/h4iIiEif0J0SHCcBWwAvuvuOwPuBV9/NLzWza4lEbwMzm2ZmR7p7G/AZ4E/AU8AN7v7Eu/k9IiIiIn1Vd7o7W9291cwwszp3f9rMNng3v9TdD+hi+x+AP7ybny0iIiKyPOhOkjbNzEYAvwNuN7OZaKyYiIiISI/qTjHbj6dvzzGzu4DhwG09GpWIiIhIP7fUMWlmtrWZDQVw98nAXcS4NBERERHpId2ZOHA5MKfweG7aJiIiIiI9pDtJmnmh4q27t9PN+moiIiIi8s50J0l7wcw+a2YD09dJwAs9HZiIiIhIf9adJO1Y4ANEgdlpwFbAMT0ZlIiIiEh/153ZndOJ6v8iIiIi0ku6M7vzIjMblro6/2Jmr5nZwb0RnIiIiEh/1Z3uzl3dfRawJ9HduT5weo9GJSIiItLPdSdJG5j+3QO41t1n9GA8IiIiIkL3Smn83syeBlqA481sJaC1Z8MSERER6d+W2pLm7mcA2wAT3H0hMA/Yq6cDExEREenPulWU1t1nFr6fS6w6ICIiIiI9pDtj0kRERESklylJExERESmhpXZ3mtlmnWx+A3jR3duWfUgiIiIi0p0xaZcBmwH/BAzYJH2/gpkd6+5/7sH4RERERPql7nR3TgXe7+4T3H1z4P3A48DOwEU9GJuIiIhIv9WdJG1Dd3+i8sDdnySSthd6LiwRERGR/q073Z3PmNnlwHXp8SeBf5lZHbCwxyITERER6ce605J2GPAccDJwCvBC2rYQ2LGH4hIRERHp15bakubuLWb2feDPgAPPpJUHAOb0ZHAiIiIi/VV3SnB8CPgpMYHAgDXM7FB3v7tHIxMRERHpx7ozJu1iYFd3fwbAzNYHrgU278nARERERPqz7oxJG1hJ0ADc/V/AwJ4LSURERES605I2xcyuAn6eHh8EPNRzIYmIiIhId5K044ATgM8SY9LuJlYhEBEREZEe0p3ZnfOBb6cvEREREekFXSZpZvYYUXKjU+6+aY9EJCIiIiJLbEnbs9eiEBEREZHFLClJ+4+7d9mSBmBmtrTXiIiIiMjbt6QSHHeZ2YlmtmZxo5kNMrOdzOynwKE9G56IiIhI/7SklrSJwBHAtWa2NtAM1AMDiCWiLnH3R3o6QBEREZH+qMskzd1biVIbl5nZQGBFoMXdm3spNhEREZF+qzt10kgLqv+3h2MRERERkaQ7y0KJiIiISC9TkiYiIiJSQktN0sys0cxq0vfrm9lH0xg1EREREekh3WlJuxuoN7MxwF+Aw4FrejIoERERkf6uO0maufs84BPA993948DGPRuWiIiISP/WrSTNzLYBDgJuTdu6NStURERERN6Z7iRpJwNnAr919yfMbB3grh6NSkRERKSfW2qLmLtPBiabWWN6/ALw2Z4OTERERKQ/687szm3M7EngqfT4vWZ2WY9H1vH7G83sp2b2IzM7qLd+r4iIiEhO3enu/A6wG/A6gLs/Cmz/bn6pmV1tZtPN7PGq7RPN7Bkze87MzkibPwHc6O5HAx99N79XREREpK/oVjFbd3+patOb7/L3XkMs4L6ImQ0AfgDsTswePcDMNgZWByq//93+XhEREZE+oTtJ2ktm9gHAzWyQmZ1G6vp8p9z9bmBG1eYtgefc/QV3XwBcB+wFTCMSte7GKyIiItLndaeUxrHAd4ExRML0Z+CEHohlDB0tZqTftRXwPeBSM5sE/L47P6i9vZ2mpiaGDx/O3LlzaWtrY9SoUcyYMYP6+npqamqYN28eI0aMYPbs2bS3tzNixAhmzpxJQ0MDAC0tLYwcOZLm5mZqamoYOnQozc3NDB48mPb2dlpbWxf9zNraWhobG3njjTdobGykra2N+fPnL3p+4MCBNDQ0MGvWLIYOHcr8+fNZsGDBoucHDRpEXV0ds2fPZtiwYbS0tLBw4cJFz9fV1VFbW8vcuXP1nkr0niaOG8KohgHc9uwcJo4bwsuzFjKj5U3Gj67n3v/MY/zoeobV1Sx6furMhbS2tbPhSnVMnjqXLcc0UFtjTJ46l53XHcJzry8AYL0VBnHH83PYYWwjbe3O319uYYexjTz96nzqa2sYO3Lgop85a368R/2d9J6W9p42W7V+mR97jzW1su2ag3msqZVRDQMYM6zj+Rktb/Ls6wvYavUGHn6llTHDahk9pJaFCxcu9T0duOnwbJ+n4ntqbm5e6t/pwE2HL/r/TXPaeHlWG5utVs8D01oYt8KgXjtHNDU1LfXYGz+6rtt/p558T9OnT1/q52nMsNplfuy9k/fU2traK+eIrpi7dyfvWebMbCxwi7tvkh7vC+zm7kelx4cAW7r7iW/3Z0+YMMGnTJmyLMMVeYuxZ9y69Bf1gqkXTModgvQBfel4Vaxvn2LtGb11fjWzh9x9QvX2pbakmdlPgLdkcu5+xDKKrWIasEbh8erAK8v4d4iIiIj0Cd3p7ryl8H098HF6Jnl6EBhnZmsDLwP7Awf2wO8RERERKb3uFLP9v+JjM7sWuOPd/NL0Mz4ErGhm04Cz3f0qM/sM8CdgAHC1uz/xbn6PiIiISF/1TtbgHAes+W5+qbsf0MX2PwB/eDc/W0RERGR50J0xabOJMWmW/v0f8IUejktERESkX+tOd+fQ3ghERERERDp0maSZ2WZL+o/u/vCyD0dEREREYMktaRcv4TkHdlrGsYiIiIhI0mWS5u479mYgIiIiItKhW7M7zWwTYtHz+so2d/9ZTwUlIiIi0t91Z3bn2URNs42J8hi7A/cAStJEREREekhNN16zD/Bh4H/ufjjwXqCuR6MSERER6ee6k6S1uHs70GZmw4DpwDo9G5aIiIhI/9adMWlTzGwE8CPgIWAO8PeeDEpERESkv1tSnbRLgV+5+/Fp0xVmdhswzN3/2SvRiYiIiPRTS2pJexa42MxWBa4HrnX3R3olKhEREZF+rssxae7+XXffBtgBmAH8xMyeMrOzzGz9XotQREREpB/qztqdLwIXAhea2fuBq4GzgQE9HJv0Q2PPuDV3CABMvWBS7hBERKSfW+rsTjMbaGYfMbNfAn8E/gXs3eORiYiIiPRjS5o4sAtwADCJmM15HXCMu8/tpdhERERE+q0ldXd+EfgVcJq7z+ileEREREQELbAuIiIiUkrdWXFARERERHqZkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJVSbOwARkaKxZ9yaOwQApl4wKXcIItLPqSVNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpIsztF+gHNmBQR6XuUpImIvENKfkWkJ6m7U0RERKSElKSJiIiIlJCSNBEREZESKn2SZmbrmNlVZnZj7lhEREREekuPJmlmdrWZTTezx6u2TzSzZ8zsOTM7Y0k/w91fcPcjezJOERERkbLp6dmd1wCXAj+rbDCzAcAPgF2AacCDZnYzMAD4RtX/P8Ldp/dwjCIiIiKl06NJmrvfbWZjqzZvCTzn7i8AmNl1wF7u/g1gz56MR0RERKSvyFEnbQzwUuHxNGCrrl5sZisA5wHvN7MzUzK3RO3t7TQ1NTF8+HDmzp1LW1sbo0aNYsaMGdTX11NTU8O8efMYMWIEs2fPpr29nREjRjBz5kwaGhoAaGlpYeTIkTQ3N1NTU8PQoUNpbm5m8ODBtLe309rauuhn1tbW0tjYyBtvvEFjYyNtbW3Mnz9/0fMDBw6koaGBWbNmMXToUObPn8+CBQsWPT9o0CDq6uqYPXs2w4YNo6WlhYULFy56vq6ujtraWubOnbv8v6eGAUwcN4SpMxfS2tbOhivVMXnqXLYc00BtjTF56lx2XncIz72+AID1VhjEHc/PYYexjbS1O39/uYUdxjby9Kvzqa+tYezIgdz27BwmjhvCrPntPNbUyrZrDuaxplZGNQxgzLCO52e0vMmzry9gq9UbmDdv3lLf08RxQxjVMGDR/3951kJmtLzJ+NH13PufeYwfXc+wuppFz/fUe2ptbV3q3+nATYfz8CutjBlWy+ghtYv+f9OcNl6e1cZmq9XzwLQWxq0wqEff0/Tp05d67O25wdBu/5168j01NTUt9fO0cuOAZX7svZP31NzcvNRzxGar1mf7PBXf08KFC5d6jjhw0+HZPk/F99Tc3LzU896Bmw7P9nkqvqempqalnsvHj67L9nkqvqfp06cv9fo0Zlhtts9T8T21trb2yjW3yxzI3ZeW87wrqSXtFnffJD3eF9jN3Y9Kjw8BtnT3E5fV75wwYYJPmTJlWf046UV9qTioYn37FGvPUKw9Q7H2jOUt1mXBzB5y9wnV23PM7pwGrFF4vDrwSoY4REREREorR5L2IDDOzNY2s0HA/sDNGeIQERERKa2eLsFxLXAfsIGZTTOzI929DfgM8CfgKeAGd3+iJ+MQERER6Wt6enbnAV1s/wPwh5783SIiIiJ9WelXHBARERHpj5SkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJaQkTURERKSElKSJiIiIlJCSNBEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpIiIiIiWkJE1ERESkhJSkiYiIiJSQkjQRERGRElKSJiIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJVSbOwDpeWPPuDV3CABMvWBS7hBERET6DLWkiYiIiJSQkjQRERGRElKSJiIiIlJCpU/SzOxjZvYjM7vJzHbNHY+IiIhIb+jRJM3Mrjaz6Wb2eNX2iWb2jJk9Z2ZnLOlnuPvv3P1o4DDgkz0YroiIiEhp9PTszmuAS4GfVTaY2QDgB8AuwDTgQTO7GRgAfKPq/x/h7tPT919O/09ERERkudejSZq7321mY6s2bwk85+4vAJjZdcBe7v4NYM/qn2FmBlwA/NHdH+7JeEVERETKIkedtDHAS4XH04CtlvD6E4GdgeFmtp67X7G0X9De3k5TUxPDhw9n7ty5tLW1MWrUKGbMmEF9fT01NTXMmzePESNGMHv2bNrb2xkxYgQzZ86koaEBgJaWFkaOHElzczM1NTUMHTqU5uZmBg8eTHt7O62trYt+Zm1tLY2Njbzxxhs0NjbS1tbG/PnzFz0/cOBAGhoamDVrFkOHDmX+/PksWLBg0fODBg2irq6O2bNnM2zYMFpaWli4cOGi5+vq6qitrWXu3Lnv6D1tOSbe03orDOKO5+eww9hG2tqdv7/cwg5jG3n61fnU19YwduRAbnt2DhPHDWHW/HYea2pl2zUH81hTK6MaBjBmWMfzM1re5NnXF7DV6g08/EorY4bVMnpI7aLnm+a08fKsNjZbrZ4HprUwboVBNDU1Lf09NQxg4rghTJ25kNa2djZcqY7JU+ey5ZgGamuMyVPnsvO6Q3ju9QU9+p7mzZu31L/TxHFDGNUwYNH/f3nWQma0vMn40fXc+595jB9dz7C6mkXP99R7am1tXeqxd+Cmw7v9d+rJ9zR9+vSlfp723GDoMj/23sl7ampqWuo5YuXGAdk+T8X31NzcvNRzxGar1mf7PBXf08KFC5d63jtw0+HZPk/F99Tc3LzUc/mBmw7P9nkqvqempqalXp/Gj67L9nkqvqfp06cv9Zo7Zlhtts9T8T21trb2Sh7RFXP3peU870pqSbvF3TdJj/cFdnP3o9LjQ4At3f3EZfU7J0yY4FOmTFlWP67P60vFbBXr26dYe4Zi7RmKtWco1p7RW0XYzewhd59QvT3H7M5pwBqFx6sDr2SIQ0RERKS0ciRpDwLjzGxtMxsE7A/cnCEOERERkdLq6RIc1wL3ARuY2TQzO9Ld24DPAH8CngJucPcnejIOERERkb6mp2d3HtDF9j8Af+jJ3y0iIiLSl5V+xQERERGR/khJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJdSjC6wvz8aecWvuEACYesGk3CGIiIhID1BLmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpISUpImIiIiUkJI0ERERkRJSkiYiIiJSQkrSREREREpISZqIiIhICSlJExERESkhJWkiIiIiJaQkTURERKSElKSJiIiIlJCSNBEREZESMnfPHcMyZ2avAi/mjkNERESkG9Zy95WqNy6XSZqIiIhIX6fuThEREZESUpImIiIiUkJK0kRERERKSEmaiIiISAkpSRMREREpISVpgplZ7hhEJC+dB0TKR0laD+lLJzxPdVjMbLyZ1eaOpyvFfWpmA3LG0h196RjoS7EWmdnossZe1ri6UjgPfMDMVsgdz/Ki6rxVlzOW5VWZr1vvlpK0HmBmVjjh7WJmYzOHtFRmdiBwOjA4dyydqdqn+wO7l/mDWRXvzmY2JndMXamK9b1lvkBXXfAOBs4CBuaLqHNV+3QvM9smd0zdYWYfA04D3swcSpeqjoHROWNZGjOrKRwHRwF7mVlpr7uVfWtmw3LH0l1m9iHgE2W+HrwbpT1Y+rLCh/I04AwKF5Ey3l2b2R7AlsDF7j6rjDEW9ukJwBeAJ929LW9UXSvEuy1wJjA7b0RdK8R6KnAhJU3UYbFYDwU2Bi5x9wV5o3qrQpynE0nPzOLzZfyMmdlewK7AZe7eXNIYi8nvycDpZrZi3qi65u7tEDfrwHuBv1W2lU1l35rZbsCXy54AA5jZusDRlPx68G4oSVuGzGxlM2tI328HfALY2d2fNbNNzOz96UOQ9eRXuFuq/P33ACYCG5jZgMpJsAzM7H2FeNcHDiXi/beZfcTMTkof1NIxs32BXwDfTsnvoNwxFRXjMbN9gL2Bj7v7S2a2upmtky+6xXVyzH4M+BwwP20vxV102m+j0vfvA/Zy9+2AF8xsm3RMUIbPWCfnoTHAjsCmZjawDDFWKyRoRwL7At9099cq592yMbMBqRX9T8Cq7v5fMxuY+xrQmXRt2hn4HnCTuzeVMU6I84CZrQ3cBcxx98f7whCYd0JJ2jKSujSPAyoHdSWrP8zMvg18G7jPzD6Q8+RXvBMF1jWzWnf/DPAz4OPAerli68LRwEoA7v4v4B7gV8AVwGHAJkTill0nJ7TfAy3AZwHcfUFZTiRmtjHww6qul4eJboNziX18uZm9N0uABVXH7KoA7v5x4Ebgd+lxW+5EzcxWBo4FFqb9+jowxMwuBC4hWtR+mLq9sqpqkdrSzFYFfky0Uu8BbFuWYxWiS8vM9its2hK4GmhMLcA/NbOL8kS3uOJ5wN3fdPeXgR2ASWa2r7svrH5dLma2ppltn5KeWmB/4Hx3v9fMPkkcr6dmDnORyj5z93Z3/zdwEbCLmb3X3UvbRf9uKElbRtx9KpGIjTOzvYG/ExfpjwK3uPuuwPlA1ibkqm7DHwIXmdnl7n4+MB34kpm9J2eM0NFi4u4nAOuY2a2p5ecK4FbgQnffm0gshueLNFRd9HY1s52AYcCmwBpmdjXESbsMFz93f5IYg7i9ma0GPEgklEcB9xM3HI8A2Vuoqo7Zy83sW2Z2qLsfDLxoZvek12Xr7kh//+nA14gbneOIJO0MYGXgmnS8nk4JxtBV7dNvE/H+EbgF+C3weWDnMhyryf+A+wut5n8GDifOYQb8hkiIs4+lKnbJm9nFqdXvCaK34qqUqJWllXJjYA7QmD4/fyCuAbcDE4B/ADuZ2SoZYwQW6479kJmdkVr/f0wcv79MN56lSH6XKXfX17v4Ii1Sn75/P5HZ/wLYo+p1BwNPAeuWIOY9iWbiUcCVwG8Lz10BXAUMLMk+/TiwOvBX4kJXfN0RwBRgk9z7tBDTaWnfXkxcSNYH6oiT3Y0liM8q+5e4YfgRcUOxcto2MP27N/AYMDZ3zCmeTwB3AysAk4HLC8/9Cbgj5z4tfL8tkYhdTyS8dYXnjgIeBzbKvT8Lsd4JNBAXupsLz51IJGuDM8dYPF5XBF4DjkuPVweGpu/3SeeCEbn3a4rn+HSc7gn8Ezgtbd8JaCeGFWSPM8U0Op2zPpHOVVsA49Jz7yVu2lbOHWeKZ0/gIeDIdE04J23/PPBima4Fy+w95w5gefkCJqUTc236gF4DfCQlQlulD+p7MsVmVY93ScnPp1MiMSht3zT9W5YP5C7pIlJJHG4Drk37eCzwA2B87jgL8Y4Dfpe+/wZwU+UinU5+/w9YrfrvkSnWA0nJOTEG5T5glfT4o+l4zXbC6+SYPTzFdWjVMbtq+nf1EuzTHYDb0vcTiZu1o4A10rHxt5Lt0/cRNxUnEolufdq+a/p3eOb9WUx+j0/H7FbA08Bn0vZ6YtjDU7nOr53E3QB8Exiajts/E62nlf27PbBBGfZtOsfuXbgWfAyoSc9NBJ4kxlWWYb8OAL5DJJV7EK3/axSe/zywfe44l/n7zh1AX/1KB8qI9P2hRLPrhwvPn0g0xe+TPqwrZIqzeKL7YEoStiJmmz1QeO4o4CdAQ8Z9ujnR7F75fjpwZNVrbiHGIdVSaKXIvW/T4zWJsX2XpDgrCVr2u2YKLSLAfsD/AesXtn2fuPNfleieW7UM+xXYKP2tdwReIGbHVZ77LNFyPSBTnGsCa6bvDyC6C/cpPL8ncbN2AnGzNqQk+3Q3ooVkReA54OnCc0cAN5M5QauKfW/gcmCd9Hgz4Fng0+nxHqSWnxLs22Hp368BjwJ/LDx3HLBn7v1ZiGfzdK76QHp8IHFTvGc6XvcGdqt+jzn2beG68D2ihfdvpFb+FO/Onf09loev7AH0xS8i0fk5cBBxh7Q/0Qz/marXfQG4NOfJuRDL54C/AOulxycA96YD/ERibFfWVimiJXJyJaEgEt8XqEpwU4KxWuZYiyfm/YkEeDDRuvdwJT4i+X2ITK2TRHfRuhS62YBjiC6Xj1W99hoi0ciS9HQS+ynpmF2V6Nr6LpGUfQA4JO3XLC1TwEjgl8Cp6YK2UUocvl/1ur2JIQUjcu/Pwj69D9gwPZ5IdCOfRXTTTinBeWBbOrqxBqdj4Kmq17yfGPN3WO59WojpZKKEzXCiW/OPwOHpuYPSZzBnMrkGMSO2kWiB/B/w/9JzlWTok0T35l65zwOFmCam47OBaK1+FDgmPbdd+txtl/vv32P7IXcAffWLmHX4E+CT6fHewPPADlWvG1mCWLchErLhhW2DiLv/XwCXARvnjjPF9Yt0cmtIjy8m7ppK0QXbSbyfTifftdPj3YBvAXcA56XnsnfDEJNWHiJ1sxCJ+ZPAtlWvWyV3rCmOPakaC0MMZD4+HR+/LEEysROR2J5CtPatDzwDnFT1uuw3aSmO8cRN0Iiq7ZsQY9K+SAnGyxFDGaYCX06PxxBdW5dXve69pJvO3F/ETcP9pO43YAjRKvlboiv5QTKPl0rH6yPAQenxxkSPyqlVrzsY2Cr3Pk2xfAj4VyUJA0YQPVd/T9eKJ4BJuePs0X2QO4C+9sXiLSiHpQOlkqgdSIyX2CV3nFUxf4DoIqwBajt7L7m/KNy1AdelC3GlRe0bxCD2lXLHWdx36eLxV1KrROG5NYguxf1yX0Sq/t7nEGPNKonaMUTi9qHc+7OTuD8CfD19P6xqvw8g78SW4rG6PdGqfnKKa+N04TijBPuwujt+bWL23mpV72FE7lgLsdSmf9cibnDOSY8rtca+nyu2pcR9YSH5qZy36tPXGsCo3DGmmHZN56xKC98mRIvkyblj6yLei4ET0/cDKv8SreubVp97l8cvleB4m9w7itG6+zVEi8lHzOyT7v4r4sN6Ya7iisXpx6mQogEvE0U/NyLVcTOzg4hq3YPKMGXZC6Up3H1/4A3g/8yswd3PJKbYN+aMsar+kROrCLwCzEjPV4rD1rn7Denrud6PFCoFVT3qh1XKmZxD1Bb7tZlt4O4/JFqCvm5mDbmOgy5+rxMXFNx9VnrdgUQX7Zueak31JjPbwMyGeKEek7vfTcyQ3ZxonXyG6P7e28xG9naMFVUlYQabWT3QRLScbEx0HVXOA180s6yrTFQ+O+l4HeDuLxKtqfuY2dketcYOB7Yws29ljrWz6+YAIgmGKGUD0TU32t1fcvcZvRJclcpnq3DN+jNwAXComR3u7o8TQzUutFghp2xeJRJdiH0MMeGlzt3/6e5PZ4mqN+XOEvvCFx1948VWtGILxWHExe5T6fGw3oqtszjT9ycR5TQuIrpiTiG6iM4jZsE8S8a7ELpoxaPQQpLi/X9knMzQxb5dk447/l8DPy08d1Da1pgx1j2IsZDDC9tqCt+fRYybe096PKIk+/XTwNdJg+/T8fsgMSblJCIByjIrLsVwB2nweif7dPt0DvgicTEZVJJ9eioxEeCXxNjEXYhZxz8kZiD+i8xDHYhJIYdWnVMrrSZjiRa1StfnqqTJGrm/iNnGHyRa+cYR45IPJ8rEVHpVss06BtZJ56PaTp6bSLSoVa5Z7yFzDxAd19nxRK3BEcSYsynEUIchRBf3I5RgCEmv7ZfcAZT9q+qEt3LVc8WTyrHEAOGhJYj5w8T4iI8Q4+ZuSxeOHYkE7UJKkqARM7U2YPEurWJXzFW5T8pVF+OT0sn38nQCNKLG0M3p7z+FVMokU6y7pZPYhzt5rvg+LiRWb8jWbVgV207EOJNz0n78Wtr+OWKM3y/JNF6KSHofpmNcTB2dDKpOn7srKU/X1rbA7cSY1FOJMhXrEInPAWlb7u743dLn6YOdPFdJ1NYiegO+kDnW4nnrAKIu10+JGd3vTYnO39K2yTkTiXROfQTYdwnvYVfiJuiIzp7PsW/TZ6gp7cPfAqsQkx3+BNwAPEDVhKfl/St7AH3li5g+fQtxp396YXvxwje8BHHuRXRp7V/Y9uN0kFfGStTkiK2TWE9PF5EbiFaTcYXn3nL3l/srJRLfJZrb9yMSyKMKz32MQktLhvi2AP5NGkibLm7HVL2meLyumHufpjgOJ2YYVmYcbkPUQ/oaHVPvsySTRKvpFODc9Hg1YgbvmC5en73VN8WxGzEr8nOFbScS4+W2yB1fimdHYvLKjoV9u2bVayqJ2ho5P1tVMR1GdBmuQMzyPZyo3zghPV9PxkSdjtncR1b24RKO192BrXPv0xTL5sRN2geIslWfS9eHNYihLuvQMZ62NOOpe/pLY9K6Ia1htj8xs+x9xIcAiDXECmOp3sgQW/V4nulEDbetzWxoiusoYnDob8sw/gzAzLYmTs67EN0EKwHPVdZf9IxL/FRL69qtT3R3tbv7I0ThxxuArczsi+5+p7v/zt1fyBhqGzGGAzPbKMU3tPiCdLxWPvev9254oZNj8AnixHxQevwAMXlkNDFeqoaOtXB722xiPOTCtObmjcAzHmOk3sLdWzrb3tM62adTiGNhKzMbDeDu3yda1q8ws/ouxlb1pg8Dr7r7XWa2FvH52r74Ak9jVT3GdeX8bBXtSUwUGeTuM4lE4nbgLDPbw91bPe8YtEOAacCN6dp0B5GMvYW7/9Hd7+/FEN8inV8HEDXQ9gNecffZxE3wrcR5bEN3f8Hdn4GOpbf6A+tH7/UdM7NDiCnh6xHN3JPcfaGZbeyxBmKuuIqDgzcHXnb3/6U1zC4lLi4/TQc8ZraKu/8vd6zp8ZbEMiQLiBagj7n7fDPbFrjP3dtzxFmIz6pPBGZ2DNGStr27P2hmjUQL2oeJ7rlcSU8N0ULWZmY7EHejqwFXuvu302ve8n5yqDpmVwMWuPtrZrYF0fLzBXe/PL2nzYEXPdbE7O04VyCuBTPSJIwjiMLUj6ebHlLykH1R56p9+iEisXycGIt6NdFV+E13/296zciUXGRnZpcRXVpjgavc/Qd5I1pc9QQMd5+Xvr+B6CreLD1ek0gw73T3V7IFHLGsQoyNbCW6vO9399Iskl5RWItzYLqeDie6OJ9098+k14wgPnv3uvsDGcPNRklalS4uznsQ0+yfcPft07ZjiQvh1919Qe9Hulh8nyO6OacSxV8vJVohvkPc4V3u7nMyxlc80Q139zfSBfp7Kc493H22mR1HJMEfydEq2UW8uxHFSu909yaLxZIvBnZ39/tSooa7z80U6+7EAObBxODql1LC/k3iTvS3lQtLmZjZ6cSg4JHA99z912a2GVF65cJKcpkptj2J8VorAhe7+zVppuYRadu97n5Lem0pkl8AMzuV6HJ/DlhIdMs+S8w+nQt8xd2bsgUImNkHiW74N939urTt60SC8/FcNzqdqToPfIaog9dC7McFZnYdUR9xq/SabEm7ma2R4nvN3R9NraenEz0/x7n7szni6kohQfswcZP7JNFlPJhUV87dT0qvLcXNUDbvtr90ef0iul7OIu6e30+Mj7kG2JAYg/AI+dbiHE7H+mp7kxaXJgZXP0AMtB5FDGa9iRIU1E3xnUzc2V9F3OGdSiSUlxKrMzyWa592Ee+JRHX2C4lB17uk7YcSFfu3zBzfRGLh9kOIMX3/IdWSI2ad3UVMaMleTJUYbF+5KTymcMzeQswwPDo93ooYVzey8vpejnOP9NkeTwxYfoBUR47okj+dSNL36e3YOol1ZToWGN8d+EP6/jvERIcriFpSQ9JnbnTmeCemz9F5RCmQrxWe+wEZJ4csJe4TiDGTaxDDSX4BrJWe+wPw1/R9rkH3GxBJzi+IYQF7p+0rpnPr+aTxcmX6IsZNPkk0MDxH3LSvkY7XR4HLcsdYhq/sAZTxi1gTcDJReflhYnWBDYmZkbcRJRZyLUezBzGu5OD0eCIxuPkzxDipbdMJ5SdEN0JZSgEcmJKGuvSB/CoxoHUHYoDo6eRfdHhtOpZz+iCxjt0gIll7jmiVnFh4P9niJWrePUVhWRyiavwWhcdbp+P3qFwXkBTHrkSiW1m4+5C0rz9H3ETsQ9SbOyU9n2VNVuLG5o/AzwrbjgDOIC1NRgxg/iJRYDnnWpyTgN8T42TriRaTselvfTsxbvb36Xw1Ieffv3AsvkDHjc4GRIvJKoXXXEIkPdmWTkpxfJBokVyfaNm5Kp1LP5ti/kX6d630+k4H5fdSrGsRraVHpMefTOeFVdLjVdJ+/T6weeb9umY6349On6NfpPPYTkT3/DVEaZhViERtm5zxluUrewBl+aLjLn9kOqiNuOO/PW0flP4dQKaZh0RJjceI5uFVC9vriBmclQTjR+k9ZFtKqfqikC4mu6V9ehtQn7YP6ez1GeLdnSgBcTAdi2avTgxkrRwDlxDJxE45Y02xrJZOal+go97ZX4gB9zcRyfrK6cK9VsY4K8fsx6qO2RWJJGLF9PgmYoBwlhqDKYZaouv4W8Dn07afEi1rrwBnEzdsdeSdvbdn2qcfIs3YTttriFa+7dPjc4lWtawtaCmWg4hW0w8VPvN/JFqlDy+87ovkTXomERNZ9ifVj0t/7/cBdxWOk9eJFsGsJWyI4SG/J27eK9eoq4l6eJXZpkPSuStn2aUNidaxk0nlVohkbW2iFFAd0Yr2KjFzthSzpMvwVUs/l2bBTXX3FjPbn+gyGkisdTnTY/YhRIXmJ9393kxxVsYYHOXuDxQrSXsMuB8B/MrMfk7cnRzgGQZcp5gmEBXjH0rjZEYQd3dfIQY17+7ubmZnEieQL+WIsyKNO/se8Gl3v7Oy3d2npQHBj6ZNfydOKk/1fpQd0t/8FTM7m7iofSKN8RoI/IoY33NmevwJzzdebj0iUTjC3R8sPucxWWAG8AUze5Lopjnd0+oCOXhMvPgj8Cawq5n9P2AWURJkZ6KQ6jHA7z3fBJzhRNHf49z9nsoMTTOr8Zi520ysKPEdIuHY2zOPQwNw91+m8ZuHAoPSuL9xxOoHHzez/YhJGafnitHMtiKS2oPc/e+V7en82gLUmtl7iBahPxITc3p95YsqvwGGEa3Vlj5z+xMrSmxnZvcT47tOyRWgxczd3wHnu/vPKts9xviOJ1qCFxDnq/uAX3mmWdKllDtLzPlFdBX+mThItiFaIUYRrSp3Ehc4iNaVJ8hbA2slYir1inReSHMQ0aX0czIuPJ326eNEMrMLUehxJFHj5nbiIrcpcQf4DzJWOydaSwcSLVKV9VcrLaqV+kzbEdPArycKBJelVlMlzrHAZUQC+Z7C8yuSubBy+pv/In3/li7M9Dn7JrGqRJnGIg5Mx/FNwJmd7feMsQ0iWqTe18V5YADwKaJFrRT7lMVr832a6NJ8kI4aePXETN5sLWgpjv2JWbCQWsgKn7OhxBja24lWzOxrRtIxLrmOqON5PTGec520ff10/tohc5wHAFcUHhuLD4X5djq3PkXcwGc/Zsv01W9b0lLryQXEwb0S0XJ2tseU+6eJqcAnmNmniAvhfp63To8RU6prPWoH1RIzpNzMNgDe7+5fMLNBnmm2adqn1xDjtv5tZl8jut3muvtMizX3diHu+mqAQzxjCROPM8RCM1tIVLmGuMi1ecdsonaim2tn4KzMx8Ai6e9e4+5T0+y4s4GPWqwt+YC7v5Y7RuKGZ7yZNbr73Errb4p9DaLm3OlmNtRTmZje1MVM7gEe5QDuII6FndL+Pcfd26pf35vS/htEnAdGesd6t+7RirYCMVb1u5WWtVyxFqXYatP+u9LM5hBDH7Yxs8c8WvoeyhwmxPG6IoCnFrLC33td4ib518R5t9M6eb0p7dcBHi19VxHHRTOwgZk1ufu/iEk5uc2vfFMpt1F4vDkxXu7XwHx3fzhDfKWWu5hhFmY2kZjuW+Pu97n7S0Qr1GkWtcT+TYzx2pcYNLyLuz+RL2Lw6Lp8Bbg1fTCLF4xtgB3MrC5jgjaRuCOaSYzrgejefBz4Xvpw/omo4bUfsWbc4zli7cJRsKjLa1Bh+0bAU+5+gadCir2tktxUSyfpGo+6TOcDmxBJxaDOXt9bCsnYFOIO+VIzG5GO18oiyR8HPpjiz5qgmVmjpQXGU+JT41FM+c+kpbOIGdXZVOL1KKVzO3C9mY33WGy+koxNIrq4GnMmaBYL0W9mZrunYRiVz9XA9P0vieWTjiMWTC9FgW0iCdvazD5e2WAdBX93IFr6/pMrQTOzkWa2upmtZ2ZDYNHxWpvO+78iJgp9jKhBWRavEkMyxqcboAHp5gJirNq4dB1WgtaZ3E15vf1FzDJ6nCig+iPihNeQnvsmUfgx26K4XcRcbBr+FXHXuQ4x0LLSbZhltmmKaTxxMduGGAtxFx3dBuum/fx9SrJOZPV+JcaYXE8aLF54/jBipuwKuWNM3x9MzCqcQKErk45ujzUozJbLEOtbBtOnY+O7xEyuUcSN4X7ExSR7uQVixt4N6e9/TGF75dgYSOqWyxTfBnQyixQ4jSgHsTsxG/GQ3OeBFNckYqLFlcRszitJyxOl54vrHR9GmuyUKdZBhe8rf+/DiBv4TxSeqxyv62aMdSOit+d6OmrfFWOsDM+oJyZpZR1G0sm204kF3TctbNuaKMGhWZxL+Op3xWxTl9xr7v5QelwZh/ZRd29NXRunEv36/80UY6XQX/Fuv9bTUklm9gNimnIDcRE51d3/mSPWFM8oYpbev9LjtYmWyIc9urPWIVp6prn7abni7Eq6w9+VqH/XRtRrGkfM5NvHM7eiwqKlyU4mlvsZQyzofounbs3c3VsWBZ/3AL7khULEqZVkQ6JEzNbAM8QNxjE5j9kU26FEcnMAUU+q1t33Ljyfe59OJJKxYzx1sxdjsiisvB1RJgKi7thjWYJlUWHlrwMnuPv9FpXvP0HUvvuru/8kvS57V6yZ7UjcnP3SC0vQmdmqxNCGLxE3w7OJVrR9PVPLv5ltSAwj+bG7/9jM3kfcEO9NjPW6Mb2uMnkkW4HldK7fBri+ar9WJrycRpQ0WUiUMTrF3X+fI9a+ot8laRXFvnEzu5ZYLLeSqJ1NzDDp9SrNVYnZyl6YoVmVqFW6X9wzzoirumgUYx9LfBgfdPcz0uP5uRLf6vg6ea6WGI/yJWKmUQNwqWcaM1e1Lz9AtER+xGNW50HEheNe4I+eaRZvRbrxuZC4WfhLYXv1UmAbAzOIMT2v9n6ki0tJ2n1EcrkbUS6knbhBey5zbHsQCc9J7v43M6tj8bGSldfVEcdrvWecEZcuzrcQy9BdWNg+imgB3hA42TOvzgKLjtfvEjPl70nbqo/VtYlkYw7wT3efminWFYiu4Vu9MPM1bd+XWPP21JJ8njYgWvrOc/dfd/Ga3YibtBpiv/4tZ1LZJ+Ruysv5RWF2FNEd8wCZCml2EttxxEnv60Rpgsr2mlwxLSXezpq41yLuRr9apviAzYhupC5rcuXcz1Wx7kvUxfs38I3C9gOIbpkDM8e6RYptUuFvfkzVa6z4b+59Wth2IjFe5sbCtmOILuWcRaDXJFpMz02PVyMq8r9l9mPOfVqIYRjRzfZ1Yszpzp28n/8C25Ug1h2JLrYdC/t2zarXlOIcS9RjG0z0QnyLqm7BdA57hEIXYsZY1yWGER2ZHg/o7HjV19v/Wu4nDnQ2KLUwsLkyOwp3P5iYWj26dyN8q9S1tT8xtuB9xAcAiMHimcJapIuBwV4YZEva9iIxQPzqDGEuxtOZw6Ke2IXEMl8Xmdm4ymuq4s92Z1eI9aNE1/vTxMSGcWZ2SnrNtUSNpLsyHxNtRKJTqTl4A1GuYJHK+6n8m0Nhnx5hZidZrL17BVE5vt7MRlmsHftZYsWBnC0+s4m/7UIzOwq4EXjGOxmwnnOfwqIaWJcQk1YuImrMTTSznSqvcff/EGVssi48nnwYeNXd70qx30HUFVykJOfYUcS4s5WJsdKvAYeb2dbp+RqPiUxPk3kCYLqeHgJMA25M19Q7iPGS8i4t90la4eS8jpnVV7ZVLshVidpR6YSS2yDgy8QJpbIsUaW7KCszm0Q0aX+auLu/MI2NwdN4iOLrPWZDvdj7kb5VOsHt6FGg+DWi9MpzqatzsZNzCS5+E0gtE+ni/CBwOVG24EsA7v5rzzdusiZ1v/+DGBR8GlGw8np3vzi9piyz9gBICe5BRFmCk4gxiJ8iyhZ8m+j23M/dsxQrNrMVzGyUu88kEsgWIjl/0t2/l14zYEk/I4MBxASBI4jaiN8H3gAmVRI1MzuAWEd4Tq4gK9z9y8ATZvYboszSD9z9F5nD6kwdMTHkQqLg94+J/Xy4mX0gnWu3A9Yjkvps0rnyCuJz9SVistXD7v7jnHEtL5bbJK3YKmJmlTXXLjGzU1MfeHsxUcsYZ2cXsteJC97h7r6rx7TlY4H9LWN5hTQw+FzgWHf/NDEe4lFgezM7HPInN0Wd7Nt24J9mdi5xQTkwxbtVdStgb+sk1tnETOMzLWqfzSLGT/2EqIM0qrdjrEjHwQ+Aq8xsDXefTCRpLwNN1lHOIvuxUNmvaezWWHf/MDHj9HngGndvd/eD3f0wYnB4rjGIexItZpPN7DB3n0GM6fw18Gp6vnJTWZrk12NCw/eI1tQvEJNavkskvtub2SXAKcSqE1lWPjCzD5rZQRYryuDuxxNdnnOIAuZl1ESca58gWtIGEonQv4G9zexE4sbiXHd/PkeAZraGmX3YzN7rsQLHeURO0ZJilWVguU3SvGMw+87EBfljRKK2NvCV6kQtl0JL30FmdpaZ7UOM37gMeMHMNkwJ0LFEK0WuOmjrEFXMb3D3+1Ps/yNOcg8R9Y6y1ucqKg5GtY5JFtOIO88diRaT+amL6xtUddH1pqpYNzSzsakr41DiJP1di4Kv84DJRJI8I1OsE4kxMv+POBnfa2YrecyWPoto+fmUpTpOuVS6sVOr+SeJSQFrmdlNwJbEckkLzOwYM/tw+m/zu/hxPR1rZZLAZ4kL83Fm9qHUovYzYrLFjunckD35TRfm4wqbGohhDQOJFsp1iUStjihmfbhnmnWajtcfEctPXW5RYLvSovYYUcNxoxyxVTOztVMLeuX6NY/oih1NzD4eRpQ0mUMs+3auu9+cI2m3mCTwJ6I1+iEzqyw/dgHRBXt45b3Iu7SsB7mV6YtoYm8n1liDuBB/mDjgL6AEg25TXJ8lLr4HE/V4jiZmQ32eWIz81+Stg9ZnBgZ3EvvJxJi4q4gLxqnp738pcef/GOVZPuckot7cr4hZchDrRX4vHQNvqZfVy/FtRCzdclhh27eBLQqPt07H8FG5Pl/Ekk6PpON2G2JmHMBeRKK+c3p8KNGiMjbjPh1FrAP5s8K2I4gi2iukx43EGq3fyH0MpHi2IMaeVZZSu4e4iRxCtJpdQSTCdURpnlxxbk10Ee6SHlcSi1UKr7mEWKZqXAn26+HEMIzKwu6/IyYMNBJJ2fXE6jeNwPrpNb3+GSMmBz1LtI4CfDKdF1ZJj1dJ+/X7wOa592tf/1quWtKKdxRmVkl0PkUsjr6jR2Xz/0cMYh1IlN3IotANM5Jo3fsQMZPndXf/EfCCu19EFIc8wPPV6OlTA4OrjoEDiQvzcUTJil2JBZR/TZy82ylPHbQDiJmcuxCxHWhmt3mMObsImErG1r7kDWIG9GiLhaYhboRONbObzGxbIvZPALd7OmP3Josp/t8GTiBKqlxMzDqDWEHgLKKb9kdEUryvZyqvkMwixhpON7PPp207EBOHHrMoB3Qg8T6+6bHiQFbu/iBR++wKM3uNGNd1RYrtZuAlYuwfnnd5snWJJHxhGjLwDPGZ360wPOMUIsmcly/M4FFH7svA78zsHmIs4mnuPpeoPvAv4vz1pqealDk+Y8Qwl6eB/1ksQ3g9MRRjvJlN8Ohh+QoxqWhuhviWK8tlnTQz25u427vSYw3JA4m7u73d/XaLCQQ1Ht1HvR3bRsBUd29JYyT+Q5yENwNmuvuk9LqjiQ/pvb0dY1Hq5jyAGGtyJfAiMZFhBNFCcWdKMD4H7OmZxp2kWKtrHR1PjD1ai0gcPuZRB2+Iu8+pfn3mWN8P/I/oNpro7h81sxeIWX27WywFlnXspLt7Stq/SLRIrUvUPPoW0S2zIXHz84l0YentGHclWiH/QiTms4lut7HAcZ4msFgUB50PtHrGun0V1lFMeVfivDWLOA52JlpSjyZq5P0vW5CdMLPxxCDx49z9OusoproWMNszdckXmdkxRGvqtcCexOSQ/yP27/PA416oP5aLLV5v8iDixmxXd3+isF/HEKvj5K7hV0eszLARsWLPekQr701EceX7ifqYF3b1M+RtyN2Utyy/gMoCxC8RY3mK9ab2I+6idswY30Tibr6eOHFUVjvYHbiTtMwH0e35BFFUswz7dSjwVeIC+B6iG+krRNfnJcDfgfGZY5xAalonujS/RiS/9xDdSZUbkjOJYouQrzuueFyuQCyWDZHgXAPslh6fQ9yxZls6p7O4iaTnsvR3f0/h+RUpLFfVy7FtT7Q07EPMiLuIaKGuIWbGXZXiLkUNrE7iH5jODzcBZ3Z1vJTti0gqZxLjJLPHU4irpvD9p4kuzQdJS3ylc/DmZKzlBYxeQszHE6tzfCD3vuwsRqIr+ziiC/bflWsVsD6RqO2QO9bl5avPt6RVDboe6DETciQxLuYWdz+x8NpPAE94hoWyraMq+3FEK8SLwNnu/jWL6tZ7EpMbZhMXk4M8UzdcGki9vrtfnh6vTNS9eYY4IV9O3IWeSYzxOzxXrCm+iURrzkeIu7pDiK6skUTr36+Ju7v3EOP8DvJyrCRwGpGsr0okZz8HziYG5LcTXcwneuYVBYoKd/WrEbFOBe509wcyx7UVgLs/YGZrEsfmG8Sx+hKRVI4kCkNnK7PTWettpZXUohTMbsBOxDFwjheW1ikrM9ucSICO9LT0UxnY4iu0HETs258Bj3nGFv8UjwG/B+a6+ycL28w7WtQ+TXzG9nb3+7IFW6VwvA4iurW3JsbP3e0ZWtCXd306Sau64B0KrA4879H0PoI4cdzq7ifni3JREvErYu3KTdO2bxB3Sxu4+//MrIGYJbUyMS4t2zIfZrYFkdQc6O7Xp/ERv0hfRxMDcK8mym8M9YzjTlLy+1Oii/ARM/sFMUFgA4/Ze7sR47zGEq0qZ3mm8X1Faabe0e6+m5n9Fljg7p+0qH30QWLcxxc936y4JS2hVezWOp8Y83Wxl2PJn1p3bzOz1YmaTcVE7RLgAs+8Jm/6vpEYUjQvPa7s04HEDdvWwEXu/nqOWN+u1F0/L8cNcPr9GxAD6kcD97l7c9peXP7vaKK18ifEdSH7UAczuzfFe1rh+WLX5wlEl+zkTLGOJPZrPfA/T2MiC5+zStfnZsA97v7zHHEu13I35S2LLyLZuZe4U5oFfCVtH05MX78wY2xbExexLYip4LcT4wog6t+8DKyeex92EvcEotXsNWLiQmX7usTF77tkXkKLOOE+QcwsOittW5u4Q70CGJi2DSEStGwz44hWsfMLj/cHPkrM4rutsi+BldO/tRljLXbHHkyMN5lAoSuTjm6PNSjMlivDFx03n6sTs3i/D6yRO65CfJ8lVma4nsISWoW4B5K65fTVrf05iZjNeyUxceVK0vJE6fnawveHUZ7hA+9L14Q5wLXF44CqbvniZ7IX49soXVevJ2Zz/og0JCc9PyD9W5+uwRvn3qfL41efnt1pYT1iTNdHiATiCWAHM/uWu79BtKD8KF+UDAcOdfcH3f1oolDtTWZW7zFg9SfAs2a2asYY38LdpxDjfAaQlkhKd3jPEy1qX3X3LHWlUizjiZlQRxF3cTua2Tfd/d9E2Y0BwLfTnfQcj6KlWWbGmdkwIlHc0My+mjbPJ8b1bUGseznfzD4H/CDdnWabJODpzGtRX+wEIvYvEkU0V0yvaU/Hw0ueYUB7mvzTKXf31FoxjRibtoBMNdCqpRb/jxL7FeLGEuhYCcXdF7q6jbrFuldgu806Cpdf4+5lmIW+GZGoX0HcyI8zs59BHAdetTRV5TPZi/FtSFybfuLRHbsvMYToeOuo1/dmOl5bgcs90xCS5V7uLPHtftHJHQUxsH0nYh1DiO6i+cDxueMtxDiw8P21pAkE6fHZlKBOTxdxl3Vg8ChSraD0eG1iVt830+N1iIkZ38oc53bEeLMhaV/+gujGhOim/RHRHXs8cXHJdjfK4i1oHyCKFK+WHh8E/JCoLbZy5n36CWKA/XpLeV2ltW9AznirYjqUGFx9MjGhpZZo5V3ie9FXp/tyHaLExheqto8iWisvAwbljrOL2CeweG28AcQ45WtzxVSIZYW0X7/ZyfZjiXF9K+WOs7989amWtKq+/I+a2VFm9j6glWh5qIyJGUosUHxzlkA74TGhobJG6AHEumyTzazO3b/q7s/mjbBzHjWRdgYuq9yZ5pbu3ma4+7/SYFs8WtCOBDYzsws8lqs5g6gvldN8oM2jFe9RojDtpqnV7HBirNSeRN2pA7wcExr2JcZHjiKtG+vuvwTuIrqYd7ZMK3WksUdfI7pZT7EoEdMpT60Rnql0SeXYrDKM6EL6oLvv7jGw/SjgSCvRih1ll1qnXyHO8w0WK8sA4FH643dEmY2tsgRYpXIspN6fWmLZpzFmtj4sOkYvAT6YjvFccdYSk1Z+l8LdpvKcx/jIu4BNiclO0gv6RJJWfbIzs5OJRZ1XIWbE7UrMNvy3md1JrCZwtkd3R6/q7MRcSCSKi7kfTFS7H927Eb59Hkv+bE4UAs7OOwbVLja43aMo6RHALmb2VXef6vkGiQ9Lf/cRxELueAysf4g4GW8JnOzu57r7ScTYpGzdBcWbH6KEydNE8jDOYmFy3P1a4qJ4l1d1x/Syz7j7ZkQZgC+b2TqVpLHwb/b1LQv79AgzO8li/d0riKr39WY2ymJ5pc8SrSrZJ170BdYHC2y7u5vZR4i//zXEuMNbicLK+1nMPt0e2MPzTb4YRbTsr0yMl36NWN5p6/R8TYrtafpI7rA86Cs7erXKN2Y2Fpjg7tsR47v+6+630rEg7XlE0dJ/5Qi0cGJepzJuJn1AFy3mXkjUjvKM5QDeDnf/R66TB0QLipltZma7W8zcXWy/VngULP04Mfs0i9S6cx0x1mSx8WXpjvkR4iS4vUVFeehoBc7GYq29rxOlH14mZkdfDmxjZl8CcPdf50p80+9/hhgbg7sfRezbrxDd3RBLlC36HOaWEtyDiBpuJxGtp58iFiD/NlFcdT93fypXjH3QAGKCwBHE3/37xCzeSZVEzaLA9nuJQflZpfPUTsSEq4uIm7ZL3f3bRCPDZsTSSld7ptncSR3Rw3MhMTzjx8R+PtzMPuAxDnU7oszR7Hxh9i+lLsGR7oaHEXXFTnP3K81sMDHtf02iS2bPlPgcTNRpyZL02OLTpj9LdBPdATwHfLuSUGRugeiTzGwSkXw/QIzfuh34u7tflZ7PtnJANYsJIa1pgsA4otXkg0SV82eJrvk2ombXCGL1iSyV5Kv3W+pm+Q6xPNkkj1UZBhPLFB1AtPxlryIPi5WsqCFm880l7vwnEeVY3sgYW6XlpI4oo3GSxZJP2xOz4xYUXlvvMfBa3gYzGwqcRnzGziOGDZxEJHDDiTI8R2ZOehYxsxOBfxAFn08HPuXuz1tHbc86j8lDOVdBqSGuqacSC9KfQnR9HkskllOJG4yvuXtphhIt70qfpKWT3fbERe6L7v6jdFe/O/BZd3/YzA4hFsveI3fLVBobMYm4CxlHjOeaThzYStTeJovZW18HTnD3+81sFWLg+FbAX71cxTP3II7Lc939VTP7MnG3P4A4wc0nkjMnWn6P9EyFaqvGoG1ILJE01aJw8TnEXfXJ7j47JWp4hmXUqlXFvWipLDP7JzGweXd3/2em2MZ5GltqMTP2TaKEiQELiSLK8y2WKnre3f9SphuMMrM+VmC7wsze6+6PWtQ724s4Fo7xWK5wH2Iy0VnEuNVeHztpUUh9BY/Z/JWGkduJc9YMIlGbTUx0ORr4tLv/XsdtL/ISzF7ozhcxJuoNor7UUKKJ+xaiSOxjFJanyRjje4lK8Vemx0OJE8alxDi50i7vUsYv+tDsLWLw/6PEqhEDCtuPJQbbbkHUExpOdCWUorYY0fpwT/oc/TRtW5WY4PBr8taW24DoCtodGFHYXl1DalNiaZpsS5MREyoeIVr+tyGKpUJcmKcBO6fHh6Zjemzuv31f+kqfnzeBT6bH96TP1hAikbiCGOdZB6yYO95C3HcSS+o1EvUyK7PPdyDqO+6WOb7DiRbojdPj3xGrtzQSCe/1RBmrRtJsel3HevlvlDuAToNavFzF/sQkAIhyBm/QscblpkT3V5ZisMWDlbjL+CRx59xKWiOUaD7enZhlWJqTR9m/0sWunjRGqnKRKzy/JvBfYLsSxLpKJRFLjwel+CuFaT8L/JbogilFUpniOiBd7BrSfl4I3JaeW50YN7dqptiWVqC0+NkbS8YCpUStsyfT33cdYoLNhem5BqI19UViUPbDlOCGsi9+0TcKbFvV4w2IUjvD07Fxb3o8mRhSUIb9eiwxZvIeFi+4vQYxk/p3pHJR+ur9r1pKxsw2Aq42s708uoKGE11DuPvfUpfSTWa2irtfljNWT0eyme1NnCiu9GjGbk8x7u3ut5vZXcBkL0F3UV+QZm+dRXRhXES09kw0s3Z3vxNi9paZlWL2FtGNuRBoTZNFziDGoTWY2WPAZ4huzhOIC3YWnXRRPE0UqTwc2NTdB5rZC2b2R3ff3czO8DxdMMUCpcUu7u3TMfCT4vvwmNWbhZntCvySqNH3FNE19Agw3szW8pjIcrWZ/T/iOGn1jBMv+jJ3n5KGvtxNVYFti+XgZnvGAtspRjezDxLnpelEF2w7MXb6l+n6tRAY7u7/zT0GzaPI9xVmNps413668NxLZnYFsUKOxk1mUqrZnWnw5FPEXemNFuuGDS++xt3vJbqUTjezEdWz+3qThUHEYOuPEOOOcPdfEeUL/mRmO7p7qxK0t6VPzd4iZur9iegmeI5o2bkO+BzRTbCTu38VOCnXya5qLNcKZjbS3f9BtEpsCfwgvfRnwNpmtlqmBG0dotX5Bne/H8BjYsV1RPmSLawk9cRSwnAp0RIxlUjOVyeS8leAs8xsbLrgPe3u/1aC9u54TATYFbjczI71jlp4L3pJJrUQ8V1BtEJtQ7QCf8HM1nH3N9K14H+QZSWBRSWfvGPiDR51EM8DfmNpJmfa/rK7P9ebMcriSjNxIN1h7EFM/3/NzK4kurQeTS95lpjBVUM0Gf/XM9QVqrrYVWbmjCS6MW5x9xMLr/0E8IRnLF3RV/XB2VtDgPFEF8FNlTt6M7sK+Ju7X5MxtuIxexpx4ViVqNf0c2LFixbijn8T4ETPMKHBokDpAmK5rzZiweY7Cs+vSczw3c/d/9bb8VUzs60A3P2BFNuZLL6g+2VEC+rp3kdK7fQVZrY5USLmSM88eagwwW0jorW0iRgbtzNx7voxMRbxvJQMZYuTWNd4rsdST5Vt5h2VCT5NnA/2dvf7csUqHUrRkmZmexKLON9BjDnAYx22x4gL9Sji4vcB4sCnBAnaocDnzWx/d58JvJ/okvtO5fXu/hslaN1jZh+2KOxZ0UDUOxtIJGfrksacEAna4WVJ0AA81ge9z91vKCRo+xKtfVkTisIxuw+wi7vvTVxIPuzuLURRzQVEi9rXMyVofapAKURylhK02hTbecQNxHHE+ep4osVkYcYwl0teogLbKUGbSIzd+jKxfvQm7n4DMaHkdaLF/9VcMVauXe6+J7C6mX0LOtYJLbSoXUkcx6VorRbyTxyg80HXw0mD7IkuhMlEvzhAbQliPp5ozdsNmAV8JW0fTkxbvjB3jH3tiz46e6uL97IqMWW9crLOFccmLD4QeH9ice8zgNtIg6zpmOCQ7bNFDKr+EtH69F6ihtxXiK7PndJrDiBaT0bn/ht3En+lV2J1ogv0+8AauePSV6/87UcSLbw7pMeHELONtym8prZ4nGSM9X3EBJY5FNYJJUqDVM+a1izOEnyVoSWtetD1F4maaDelQYsXEuOTfmdRqT9bjbE0Bm09YrbmR4jWnSeAHczsWx4FNMcSHwJ5GzzWCN0KuMLMXgN+4O5XeKx5eTPRfXRQeu1r+SLtlmaie34vd388RwCp63AIsKFFYV2Iz9pXiIR4kkfNrs8BP7AovJpljUsAj7VWv0e0NnwBGEO0nDYTEwYuIZL1I9y9KUeM6fzUKXf31FoxjWgJXEDsb1mOpUkCQ4hCtdMtavf9nLjZOK7QQtWW/s02vsjMNgNuIG54tyaWfPtZJS6vqt+ZM1bpUIYkrZmuB10PIUovHA4c5u5vVh9IPS312QOLDuTngAOJO5J93X0bokTEiWZ2vLvPcg20fEc8CipuT4w7W2z2FvAL4KueefZWd7h7i7vfmus4sFi65XtEXaZvAOua2Rfd/bfEpJwZwE5mdjwxVuZsd5/f2yflvtTFncaXXp9u0jqVErUaj67Pz3umQsXSO8xsS6JMzVrEBKFP0tGI8Awx27RMhctrgPvd/aF087gV0cBwbea4ZAmyl+BIJ7YribEF1YOujyHuqCHNhulNVWPQPkosPDuFaD17k471FocSC09rqYx3yd0fsyhr8GczG+HuV6TtL2YOrS+ZT1Qwn2NmjxIJ2+dSq9nhxLiZPYkuxQM838Lus4BLzWyGu19PfIYuIxLyo4lyAFcTN0EX52pBtVgu62vEfj3FzC5OLX9v4R2z4rK1SkrPs1hD+gvAb9z9HjN7kSj8umrqCZhEfM6yKUxoMOLGtwkYY2bru/u/PJZTvAQ41cw2cI2fLqXSzO6slgZdf4EYo/R8L//uRQd3+vdkYG+ixe+TxJieR4kP4frEcjT7eqZF3ZdHZZq91VekLs7ZRIHnE9x9r7R9AFEI9BRizdNvp+11uVsmLRZ1v5246TnR3a9N29clxtCtTLRKZYszJWmruftdZvZjokXi68S6q+3WsY6olsrpJ8xsPNHbsxFwdLq5XIkYpzwUeNLdJ+c6JgrXro8QN2SNRO3JjxGt1d8nWqz3JlrSSzMJSxZXuiTNzFYlEqGjiQSt18f0mNkYd3853YGsRcx4Ozh1zXzc3Xc1s4HEorMbAS+4+797O87lnZm9H5inO7yls6gvdinR4rMi0TX4icLzdcREgq8A/3D3r5YlqUgXvLuB49z9ukLSsxbRZZS9/pWZDU9jTjGzHxG9EF/3KKQ61jMW1JWeV0h6NiFWQnmWWFXkCKL6wJUZW6Q7lWZGn0+M5b0MeNPd90g9VOsQi6j/0N1vyRimLEUZk7QGYCfgmd4e05OSsmHEWnunufuVFotLn0/UbGsgKke/aWYHA3e76h9JZmZW7+6taYLAOKLF94PEBJxniWXK2ohZaCOIFqBeHz6wJGa2BfBn4MxKF3fZFJLHGqJA6VyiGPAkYGIliZPlk5ntTHS/30/c8BwPzCMmko0huuRLc0NpZicSExpWBE4HPpVuKir1Pes8Jg+V4mZNOpd9TFo176jblOv3v2Fmk4D/s1iC5kdm9irRXfTZlKAdQnTF3p0rThFYVAR6dzM7193PNrMvE8UoBxBd8fOJ5MyJMSlHlnFAu7s/mC6CD5rZ/DJ1cVcuYilBG5DGmx1tZv8khjrsrgRt+WZRqPZAYB93/3tqjToN+DyxSseRxGcuOzN7r7s/SkxiOIsor3Gwx5KF+xCrdpxF3LhpFmfJla4lrSzSmKg7icHLtxKtaWsTg53HA/u7+xP5IpT+zqII9HlEUvb7ymB1MzuWGDLweaIgdKW8xpCytaBVy93FncafNQKjgfvcvTltrynO1DOzTYGbgI9qPM/yK7Wa1hGzjLclurgr4ybPBca7+8fNbKi7z84Y6iJmdidRQPsion7bH939dDPbgSi/cbK7/ylnjNJ9StLoWN4pfb8/sEEas7MdcAsxvuc36cQ8GnjKox6SSBYWi45fSwyqf9BiPct6oN7dp5vZZ4EdidI2D3qGFTr6mtSCfh5xYduFmNDwd3e/Kj1fnO09Fljg7qVY/UCWrcIYtEZ3n2tmKxCtUvOAG939oZT0HAUc6hlLbVR3V6YbjS8CnyVaen9OR3mri9w9W0+VvH2l6+7sbakZ+2oz2yt1Aw0nuoVw97+l7qSbzGwVd78sZ6wiBdVFoM8gxqE1mNljxCLfI4ETiMHNsgRmtjtwLnCsu9+fkuBPEIV02939J8ULoSYKLN9SgvZRYmH0e4iE/StEcfXz02dsGyLpyVoLLcX6QWK5tOnA80RX557u/st0DVsIDHf3/2oMWt9ShmK22aSBk08RBT5vtFgofXjxNe5+LzFt+XQzG5Gav0Vya6brItCNxFJKXwVOcvfWTDH2CWlm7MXADe5+P0DqFr4OeIgYw6O1DPsRM1sR2I+YMf0k0Sq1PXAq8C+ihep8d78pW5CL25XoyvwakTxeSSSY67j7G+4+j1RrVAla39JvuzvT3cUewDnu/ppFQd01ifpnELPi5hKJ7L3Af9VlJGViZkOI8ZHVRaCvAv7m7tdkDK9PsKgtt4CoedgG3OPudxSeX5Po/tzP3f+WJ0rpTRa1+7YFVnL3L6cZ/rsShaB/RtwcXUTUJPxpjtIbhe7YjYhW9SZi7NzORJf9j4nVRM5z91/2dnyy7PTLVqE04PobwB3ATAB3/zQxyPo0ou7NGsAHiIMeJWhSNu4+x93vc/cbCgnavsQC5UooliLVYbuEKKdwETG5YmKqLwVAKrFzK9GVJMspi4LPlbU4f04smXSwmW2RWqFuo2MljHbiuBlErDXb61KCNhH4HXGD8QSwibvfAEwEXicWUc8Snyw7/a4lrYsB1w3AwNSidgZR92aiu7eYWa2nxXFFyspKUAS6r0ndnAcQNa6uBF4ETiRqyd3q7nea2QFEF/Kenmlhd+k5VUWKxwPfAU5394fN7Bzg/cDX3H1KGvs5vHIc5Lw2pKE5txHXscmpLNS5wIHufl8xPo1B69v6Y0ta9YDrLxJFP28ysyuIgaEvAL9Ld1dlWiBXpCvNRBf9XkrQusdj/c3vEa0NXyCSte8S+3J7i3UNTwGOUIK2/LFYheNhMzslbVqVqC14AIC7n0OMSbwwtai1Fo+DjAnaB4EhRKHa6al238+JVQWOq4ybrsSnBK1v649JWjNdD7geAuzs7ocDh7n7m7ln7oh0h7u3uPut3surdPQ1ZvZhi+XdKhqItQwHAicB6xKJWh0xLulw1UFbPqUhAgcDZ5rZUe7+Z2Im9AYW6zXj7ucSY5JLkeiY2ZbAN4nlChuJ1vPKNeoZYhk1XbOWI/2uuxO6N+BaTcQiyx+L5afuJ7qFrk/lFX5Bx3ijDYilfx4Fhrr7a9mClV5hHUuSfd5jhZldgGOJGnkX5o2uQ6rNdzFwv7t/08zWAK4nxlJXlif7smstzuVKv0zSOpMGXH+BGM/zfO54RKRnpNl7txMTBU70jgry6wL7AysTF+z5+aKU3lRI1E539x+nunknEPUGXyzDDXsaM/c5YCPgaHd/zMxWAnYDhgJPpvFpamBYjvT7JE0DrkX6n3TBuxs4zt2vs47F09ciuoxmZA5RellK1G4Fznb3y81sxZwtqYUyG5sQq4k8CwwjumRHAVfmKP8hvUtJmlkDsBPwjMbziPQfhdaTM939itzxSH5mtjVRmmkjYFruFikz25nofr+fKBVzPLE01e7ERJeLPdM6t9I7+n2SJiL9l5ltDjwIHOnuP8kdj+RXLMuROY6NgNOBK9z972Z2DPBR4PNAC3Ak8Cu1pi3flKSJSL9mZu8H5qlFQmCxbsYsY7tSCY06YpbxtsDXC+MmzwXGu/vHzWyou8/u7fikd/XHEhwiIou4+z+UoElFJTHr7QTNzCx92+DuLcCZRNfrpqnFF+AvwJw0hlIJWj+gljQREZESMLOPElUG7iFmIP+dKLC+DlFqYxvgIi/Pwu7Sw5SkiYiIZGZmKxLLUt1KrAu6N/BD4E4iURsC3Ojut+aKUXpfbe4ARERE+rNUu29bYKq7X2tmg4E3iNJQdUTX50XEcmX/1mSB/kNj0kRERHpZWhu6shbnz4GtgIPTOqHziAXUKythtAOXEC1sr+aJWHJQd6eIiEgvKZb4SEWVv0OsdPCwmZ0DvB/4mrtPMbN6YHhlYXczq821sLvkoZY0ERGRXmBmdcDDZnZK2rQqsD5wAIC7nwM8BFyYWtRaKwlael4JWj+jJE1ERKQXpPVgDwbONLOj3P3PxDJPG5jZyek15wL3AurmEk0cEBER6S3ufp+ZTQL+nArm/iiVSDvWzOrc/UJ3PytzmFISStJERER6kbs/aGa7Eomau/uPzawWOMHMrgdezL1uqJSDkjQREZFeVkjUbjWzge5+uZk96O6v5Y5NykOzO0VERDIxs62J5Z82AqapBU2KlKSJiIhkVCzLIVKk2Z0iIiJ5zYLFFlkXAdSSJiIiIlJKakkTERERKSElaSIiIiIlpCRNREREpISUpIlIn2dmq5jZdWb2vJk9aWZ/MLP1u3jtCDM7vpfiOtbMPtUbv0tElj+aOCAifVqaEff/gJ+6+xVp2/uAoe7+t05ePxa4xd036eG4arUgtoi8G2pJE5G+bkdgYSVBA3D3R4B/mNlfzOxhM3vMzPZKT18ArGtmj5jZNwHM7HQze9DM/mlmX638HDP7ipk9bWa3m9m1ZnZa2v4+M7s/vf63ZjYybf+rmZ1vZpOBk8zsnML/WdfMbjOzh8zsb2a2Ydq+r5k9bmaPmtndPb+7RKSv0LJQItLXbQI81Mn2VuDj7j7LzFYE7jezm4EzgE3c/X0AaWmeccCWgAE3m9n2wDxgb+D9xLny4cLv+RlwortPNrNzgbOBk9NzI9x9h/SzzynE80PgWHd/1sy2Ai4DdgLOAnZz95fNbMS73BcishxRkiYiyysDzk8JVzswBhjdyet2TV//SI+HEEnbUOAmd28BMLPfp3+HE4nY5PT6nwK/Lvy8698SiNkQ4APArwv1SuvSv/cC15jZDcBv3v7bFJHllZI0EenrngD26WT7QcBKwObuvtDMpgL1nbzOgG+4+5WLbTQ75R3GM7eTbTVAc6X1rsjdj00ta5OAR8zsfe7++jv83SKyHNGYNBHp6+4E6szs6MoGM9sCWAuYnhK0HdNjgNlEK1nFn4AjUmsXZjbGzFYG7gE+Ymb16blJAGmNxZlmtl36/4cAk1kCd58F/NvM9k2/w8zsven7dd39AXc/C3gNWOMd7wkRWa6oJU1E+jR3dzP7OPAdMzuDGIs2FTgH+J6ZTQEeAZ5Or3/dzO41s8eBP7r76Wa2EXBf6oqcAxzs7g+mMWyPAi8CU4DKItiHAleY2WDgBeDwboR6EHC5mX0ZGAhcl372N81sHNGi95e0TUREJThERLpiZkPcfU5Kxu4GjnH3h3PHJSL9g1rSRES69kMz25gYy/ZTJWgi0pvUkiYiIiJSQpo4ICIiIlJCStJERERESkhJmoiIiEgJKUkTERERKSElaSIiIiIlpCRNREREpIT+P051fgbVhQbAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.bar(training_times['Name'], training_times['Training Time'])\n",
    "plt.yscale('log')\n",
    "\n",
    "# Set yticks at your specified values\n",
    "yticks = [10**(-2),10**(-1),10**(0),10**(1),10**(2),10**(3)]\n",
    "plt.yticks(yticks)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values (log scale)')\n",
    "plt.title('Bar Chart with Log Scale')\n",
    "\n",
    "# Add a grid only at major ticks\n",
    "plt.grid(axis='y', which='major', color='lightgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Remove minor ticks\n",
    "plt.tick_params(axis='x', which='both', bottom=False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f95cc3-3740-4a2d-a7a3-db152face017",
   "metadata": {},
   "source": [
    "### Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f44df-f490-47a7-b9e3-09ae28d4f136",
   "metadata": {},
   "source": [
    "Our goal was to hard-code a random forest classifier using only NumPy and pandas. We used two toy data sets (the iris data set and the breast cancer data set) to compare our hard-coded classifier to scikit-learn's RandomForestClassifier. The iris data set had shape (150,5), which means that this data set is quite small. The breast cancer data set had shape (569, 31), which is many more columns than the iris data set but still not many rows. Excluding the target, all variables in these data sets were continuous. To make sure that our hard-coded model could handle boolean variables, we converted one of the iris data set columns to a binomial variable. \n",
    "\n",
    "As an intermediate step, we hard-coded a decision tree classifier. The training time of this classifier was 0.42 seconds on the iris data set, and 58 seconds on the cancer data set. Of course, these times might vary on different machines. When we compare these training times to scikit-learn's DecisionTreeClassifier, we see that our model's training time is roughly $10^2$ times higher on the iris data set, and roughly $7*10^3$ times higher on the breast cancer data set. On the iris data set, the prediction of both models were the same, and the accuracy of both models was 96.67%. On the breast cancer data set, 99% of the predictions were the same (1 prediction was different), and the accuracy was 93% for scikit-learn's decision tree and 92% for our hard-coded decision tree. This difference is negligible once random forests are trained.\n",
    "\n",
    "Next, we modified our hard-coded decision tree classifier to build one of the trees that are part of a random forest. The fundamental difference between this tree and a standard decision tree is that at each step, only a subset of the variables is used to obtain the next best split. Since not all columns are considered at each step, these trees are trained faster than a standard decision tree. Specifically, we found that their training time is roughly 3 times lower on the iris data set, and 6 times lower on the breast cancer data set. \n",
    "\n",
    "Using the our modified decision tree, we hard-coded a random forest classifier. The training times for a random forest of 100 trees were roughly 21 seconds on the iris data set, and 23.5 minutes on the breast cancer data set. We believe that the reason for the very long training time on the breast cancer data set is due to the fact that the dimension of the data set is larger (in terms of both rows and columns) than the iris data set and that all variables are continuous, so that the model has to check roughly $450*2/3 = 300$ (the number of rows in the training set times the approximate probability that each data point is part of the bootstrapped data set that the tree is trained on) unique values to obtain the best split for each column. \n",
    "\n",
    "Comparing these results with the training time of scikit-learn's RandomForestClassifier, we found that the training time of our model is $1.4*10^2$ times larger for the iris data set, and $7.4*10^3$ times larger for the breast cancer data set. These results are consistent with the training times for decision trees. In addition, the predictions of both models were identical on both data sets. The accuracy on the iris data set was 100%, and the accuracy on the breast cancer data set was 95.61%. These results indicate that although the training time of our hard-coded model is much higher, the predictions of our model are completely equivalent to the ones of scikit-learn's RandomForestClassifier.\n",
    "\n",
    "In an attempt to reduce the training time of the hard-coded models, we wrote a different function to train the decision trees of the random forest. The main difference between this new function an our old one is the way the best split for each column is determined. In the old function, the unique values in a column are determined, and for each unique value, the weighted Gini score is calculated so that the best split can be found. In the new function, the values unique values in the column are ordered and distributed into two bins, with one of the bins initially empty. Then, at each iteration the weighted Gini score is calculated, and the  lowest value in the initially full bin is moved to the initially empty bin. The advantage of this is that the data is split only once rather than at each iteration. With these new functions, we trained a random forest of 100 trees in 19 seconds on the Iris data set, and 21.3 minutes on the breast cancer data set. Thus, although there is a small improvement, the training times are very similar to the ones of our old function. The predictions are also essentially the same, with an accuracy of 96.5% on the breast cancer data set. \n",
    "\n",
    "In conclusion, we were successful in building a hard-coded random forest classifier. The predictions of our models are essentially identical to the prediction of scikit-learn's models, but the training times of our models are orders of magnitude larger. We managed to slightly improve the training time of our models, but this improvement is negligibly small. In order to significantly improve the training time of our models, we should try to vectorize the processes in our functions as much as possible. In addition, we might try to find a good split rather than the best split. This would likely not affect the accuracy of our models too much in large enough forests. Finally, we should try to train trees in parallel rather than one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325207a7",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d68c8b",
   "metadata": {},
   "source": [
    "- Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, *An Introduction to Statistical Learning: with Applications in R* (Springer, 2013).\n",
    "- Trevor Hastie, Robert Tibshirani, and Jerome Friedman, *The Elements of Statistical Learning*, Springer Series in Statistics\n",
    "(Springer New York Inc., New York, NY, USA, 2001).\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- https://stats.stackexchange.com/questions/105487/the-efficiency-of-decision-tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
